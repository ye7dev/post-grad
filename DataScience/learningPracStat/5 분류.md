# 5. 분류

- 용어 정리
    - 조건부 확률(conditional probability): 어떤 사건(Y=i)이 주어졌을 대, 해당 사건(X=i)을 관찰할 확률. $P(X_i|Y_i)$
    - 사후확률(posterior probability): 예측 정보를 통합한 후 결과의 확률(↔ 사전확률-예측변수에 대한 정보를 고려하지 않음)
    - 공분산(covariance): 하나의 변수가 다른 변수와 함께 변화하는 정도(유사한 크기와 방향)를 측정하는 지표
    - 판별함수(discriminant function): X에 적용했을 때, 클래스 구분을 최대화 하는 함수
    - 판별가중치(discriminant weight): 판별함수를 적용하여 얻은 점수. 어떤 클래스에 속할 확률을 추정하는 데 사용
    - logit(로짓): (0~1이 아니라) += inf 범위에서 어떤 클래스에 속할 확률을 결정하는 함수
    - odds(오즈): 실패(0)에 대한 성공(1)의 비율
    - 로그 오즈(log odds): 변환 모델(선형)의 응답 변수, 이 값을 통해 확률을 구한다
    - 정확도(accuracy): 정확히 분류된 비율
    - 혼동 행렬(confusion matrix): 분류에서 예측된 결과와 실제 결과에 대한 레코드의 개수를 표시한 테이블
    - 민감도(sensitivity): 1을 정확히 1로 분류한 비율 (재현율, recall)
    - 특이도(specificity): 0을 정확히 0으로 분류한 비율
    - 정밀도(precision): 1이라고 예측한 것들 중에 1이 맞는경우의 비율
    - ROC 곡선: 민감도와 특이성을 표시한 그림
    - 리프트(lift): 모델이 다른 확률 컷오프에 대해 비교적 드문 1을 얼마나 더 효과적으로 구분하는지 나타내는 측정 지표
    - undersample: 분류 모델에서 개수가 많은 클래스 데이터 중 일부 소수만을 사용하는 것 (과소표본, downsample)
    - oversample: 분류 모델에서 희귀 클래스 데이터를 중복하여, 필요하면 부트스트랩해서 사용하는 것(과잉표본, upsample)
    - up/down weight: 모델에서 희귀(혹은 다수) 클래스에 높은(낮은) 가중치를 주는 것
    - 데이터 생성: 부트스트랩과 비슷하게 다시 샘플링한 레코드를 빼고 원래 원본과 살짝 다르게 데이터를 생성하는 것
    - z 점수: 표준화 결과
    - k: 최근접 이웃 알고리즘에서 이웃들의 개수

---

- 주요 개념
    - 나이브 베이즈는 예측변수와 결과변수 모두 범주형(요인)이어야 한다
    - 나이브 베이즈에서 답하고자 하는 질문 - ‘각 outcome(y) 카테고리 안에서, 어떤 예측변수(X_j)의 카테고리가 가장 가능성이 높은가’
        - $P(X_1, ..., X_p | Y=i)$ → $P(X_1|Y=i) *...*P(X_p|Y=i)$
    - 이 정보는 주어진 예측변수 값에 대해, 결과 카테고리의 확률을 추정하는 것으로 바뀐다
        - $P(Y=i|X_1, ..., X_p)$
    - 판별분석은 예측변수나 결과변수가 범주형이든 연속형이든 상관없이 잘 동작한다.
    - 공분산행렬을 사용하여 한 클래스와 다른 클래스에 속한 데이터들을 구분하는 선형판별함수(linear discriminant function)을 계산할 수 있다
    - 이 함수를 통해 각 레코드가 어떤 클래스에 속할 가중치 혹은 점수(각 클래스당 점수)를 구한다
    - 로지스틱 회귀는 출력이 이진변수라는 점만 빼면, 선형회귀와 매우 비슷
    - 선형모형과 비슷한 형태의 모델을 만들기 위해, 응답변수로 오즈의 로그값을 사용하는 등의 몇 가지 변환이 필요
    - 반복 과정을 통해 선형모형을 피팅하고 나면, 로그 오즈는 다시 확률값으로 변환된다
    - 로지스틱 회귀는 계산 속도가 빠르고 새로운 데이터에 대해서도 간단한 산술연산으로 빠르게 구할 수 있다는 장점 때문에 많이 사용된다
    - 정확도(예측한 분류 결과가 몇 퍼센트정확한지)는 모델을 평가하는 가장 기본적인 단계다
    - 다른 평가 지표들(재현율, 특이도, 정밀도)은 좀 더 세부적인 성능 특성들을 나타낸다(예를 들면 재현율은 모델이 1을 얼마나 정확히 분류하는지)
    - AUC(ROC 곡선 아래 면적)는 모델의 1과 0을 구분하는 능력을 보여주기 위해 가장 보편적으로 사용되는 지표
    - 리프트는 모델이 1을 얼마나 효과적으로 분류해내는지를 측정. 가장 1로 분류될 가능성이 높은 것부터 매 십분위마다 이를 계산
    - 데이터의 심각한 불균형(관심 있는 결과의 데이터가 희박할 때)은 분류 알고리즘에서 문제
    - 불균형 데이터를 다루는 방법 - 다수의 데이터를 다운 샘플링하거나 희소 데이터를 업샘플링
    - 갖고 있는 1의 데이터를 모두 사용해도 그 수가 너무 적을 때는, 희귀한 데이터에 대해 부트스트랩 방식을 사용하거나 기존의 데이터와 유사한 합성 데이터를 만들기 위해 SMOTE 사용
    - 데이터에 불균형이 존재할 경우 보통은 어느 한쪽(1의 클래스)을 정확히 분류하는 것에 더 높은 점수를 주게 되어 있고, 이러한 가치 비율이 평가 지표에 반영되어야 한다

---

### 5.0 개요

- 자동으로 어떤 결정을 해야 하는 종류의 문제
    - 스팸 메일, 고객 이탈, 광고 클릭 등
- 분류
    - 지도학습의 한 형태
        - 결과를 알고 있는 데이터 → 이걸로 모델 학습 → 결과가 알려지지 않은 데이터에 모델을 적용
    - 데이터가 1/0 혹은 여러 카테고리 중 어디에 속할지 예측
        - 범주의 개수가 두 가지 이상인 경우 각 클래스에 속할 확률 예측
- 대부분의 알고리즘은 관심 클래스에 속할 확률 점수(경향, propensity)를 반환
    - R - 로지스틱 회귀 - 기본 출력: 로그 오즈 척도 → 경향 점수로 변형
    - Python - 사이킷런 - 로지스틱 회귀 - predict(클래스를 반환), `predict_proba`(각 클래스에 대한 확률을 반환)
    - 경향 점수가 나오면 컷오프(절사)를 통해 결정을 내릴 수 있음
- 일반적인 접근 방식
    1. 어떤 레코드가 속할 거라고 생각되는 관심 클래스에 대한 컷오프 확률을 정한다 
    2. 레코드가 관심 클래스에 속할 확률을 어떤 모델을 가지고 추정한다 
    3. 그 확률이 컷오프 확률이 상이면 관심 클래스에 이 레코드를 할당한다 
    - 컷오프가 높을 수록 관심 클래스에 속할 레코드 수가 감소. vice versa
- 범주 항목이 두 가지 이상이라면
    - 조건부 확률 사용해서 여러 개의 이진문제로 돌려서 생각하기
        - 예) Y = 0 vs. Y > 0 → Y > 0 이면 Y = 1 or Y = 2
    - 특히 하나의 카테고리가 다른 카테고리보다 훨씬 더 일반적인 경우

## 5.1 나이브 베이즈

- 나이브 베이즈 알고리즘
    - 주어진 결과에 대해 예측변숫값을 관찰할 확률을 사용
    - 예측변수가 주어졌을 때, 결과 Y = i를 관찰할 확률. 즉 정말 관심 있는 것을 추정
- 완전한 베이즈 분류에서 각 레코드 분류
    1. 예측변수 프로파일이 동일한(예측변수 값이 동일한) 모든 레코드들을 찾는다
    2. 해당 레코드들이 가장 많이 속한(즉, 가능성이 가장 많은) 클래스를 정한다. 
    3. 새 레코드에 해당 클래스를 지정한다. 
    - 표본에서 새로 들어온 레코드와 정확히 일치하는 데이터를 찾는 것에 무게를 두는 방식

### 5.1.1 나이브하지 않은 베이즈 분류는 왜 현실성이 없을까?

- 예측변수의 개수가 일정 정도 커지면, 분류해야 하는 데이터 대부분은 서로 완전 일치하는 경우가 거의 없다.
    - 예) 지난 선거에서 투표한, 미국 중서부 출신, 히스패닉계, 남성, 고소득자, 더 이전 선거에서는 투표 경험이 없고, 딸 셋과 아들 하나가 있으며, 현재 이혼한 상태에 있는 레코드가 새로 들어왔을 때 - 완전 일치하는 데이터가 없을 수도
- c.f. 나이브 베이즈는 베이즈 통계의 방법으로 간주되지 않음
    - 나이브 베이즈는 통계 지식이 거의 필요 없는 데이터 중심의 경험적 방법
    - 베이즈 규칙 비슷한 예측 계산이 들어가다보니 이름을 그렇게 붙였을 분
        - 결과가 주어졌을 때, 초반에 예측변수의 확률을 계산하는 부분과 결과 확률을 최종적으로 계산하는 부분이 그렇다

### 5.1.2 나이브한 해법

- 나이브 베이즈 방법에서는 확률을 계산하기 위해 정확히 일치하는 레코드로만 제한할 필요가 없다.
- 대신 전체 데이터를 활융
    1. binary response(Y=i, i=0 or i=1) 에 대해, 각 예측변수에 대한 조건부확률 P($X_j|Y=i$)를 구한다 
        - Y = i가 주어질 때, 예측변수의 값이 나올 확률
        - training set에서 Y = i인 레코드들 중 $X_j$ 값의 비율로 구할 수 있다
            - $P(X_j \cap (Y=i)) \over {P(Y=i)}$
    2. 각 확률값을 곱한 다음, Y = i 에 속한 레코드들의 비율을 곱한다
        - 각 확률값 = 각 예측변수에 대한 조건부 확률
    3. 모든 클래스에 대해 1~2단계를 반복 
        - 여기서는 binary class니까 Y=1, Y=0
    4. 2단계에서 모든 클래스에 대해 구한 확률값을 모두 더한 값으로 클래스 i의 확률을 나눈다 → 결과 i의 확률을 구할 수 있다
        - 분자: 클래스 i에 대한 2단계 값
        - 분모: 클래스 0에 대한 2단계 값 + 클래스 1에 대한 2단계 값
    5. 이 예측변수에 대해 가장 높은 확률을 갖는 클래스를 해당 레코드에 할당한다 
- 예측변수 $X_1, ..., X_p$가 주어졌을 때의 출력 Y = i 의 확률에 대한 방정식으로 표현 가능
    - $P(Y = i | X_1, X_2, ..., X_p)$
- exact naive bayes 사용하여 클래스 확률을 계산하기 위한 공식
    
    ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/082ca6e5-a804-4cf7-afc5-182865ae4dc8.png)
    
    - 위를 조건부 독립성에 대한 나이브 가정하에서 전개한 결과
        
        ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/08388aa6-2d2f-4ace-8c58-5919ad1907b8.png)
        
- 나이브한 공식의 의미
    - 결과가 주어졌을 때, 예측변수 벡터의 정확한 조건부 확률이
        - 각 조건부 확률 $P(X_j | Y = i)$의 곱으로
        - 충분히 잘 추정할 수 있다는 단순한 가정을 기초로 하기 때문
    - $P(X_1, X_2, ..., X_p | Y=i)$  대신, $P(X_j |  Y=i)$를 추정하면서
        - $X_j$가 k ≠ j인 모든 $X_k$와 서로 독립이라고 가정한 것
- 💚 R-klaR-naive_bayes 결과 해석
    
    ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled.png)
    
    - Y(outcome)이 특정 값일 때, 각 예측변수(X_j, 칼럼)이 특정 카테고리에 해당할 조건부 확률
        - $P(X_j | Y= i)$
        - 예) outcome이 paid_off일 때 purpose 칼럼 값이 credit_card일 확률이 0.18759649
    - 예측변수가 3개고 $마크 달린 칼럼 이름도 그래서 3개가 나옴
        - 거기에 var로 묶인 칼럼들은 특정 예측변수의 요인수준들
- 나이브 베이즈 한계
    - 편향된 추정 결과 예측
    - 그러나 Y = 1인 (채무 불이행) 확률에 따라 레코드 들에(고객 데이터) 순위를 매기는 것이 목적이(누가 빚을 못 갚을 확률이 높은지)므로 확률의 비편향된 추정치를 굳이 구할 필요가 없다면, 쓸만하다

### 5.1.3 수치형 예측변수

- 베이즈 분류기는 예측변수들이 범주형인 경우에 적합
- 수치형 변수에 나이브 베이즈 적용하기 위해서는 두 가지 중 하나를 따라야 한다
    1. 수치형 예측변수를 binning 해서 범주형으로 변환 → 알고리즘 적용
    2. 조건부 확률 ($P(X_j | Y = i)$)을 추정하기 위해 정규분포와 같은 확률 모형을 사용 
- train 데이터에 예측변수의 특정 카테고리에 해당하는 데이터가 없는 경우
    - 다른 기법들: 이 변수를 무시하고, 나머지 변수들의 정보 사용
    - 나이브 베이즈 알고리즘: 새 데이터 결과에 대한 확률을 0으로 할당
        - 예시
            - 훈련 데이터에서 X_j 칼럼의 범주가 [A, C]이고, 범주 A의 빈도가 3, 범주 C의 빈도가 2라고 가정
            - 나이브 베이즈 분류기는 이 정보를 사용하여 조건부 확률 P(X_j=A|Y)와 P(X_j=C|Y)를 계산합니다.
            - 이제 새로운 데이터에서 X_j가 B일 때, 훈련 데이터에 B가 없으므로 P(X_j=B|Y)는 0이 됩니다.
        
        → 이를 방지하기 위해 평활화 인수(라플라스 평활화) 사용 
        

## 5.2 판별 분석

- 트리 모델이나 로지스틱 회귀와 같은 더 정교한 기법이 출현한 이후로는 LDA를 그렇게 많이 사용하지는 않음

### 5.2.1 공분산 행렬

- 공분산: 두 변수 x, z 사이의 관계를 의미
    
    $s_{x, z} = {\sum_{i=1}^n (x_i-\bar{x})(z_i-\bar{z})\over{n-1}}$
    
    - 양수는 양의 관계, 음수는 음의 관계
    - c.f. 상관관계는 공분산을 두 변수 각각의 표준편차의 곱으로 나눈 것
        - 상관관계는 두 변수의 단위에 영향을 받지 않음
        - 공분산의 크기를 두 변수의 변동성으로 표준화한 값이므로 단위가 없는 척도
    - 공분산 행렬: 각 변수의 분산 $s_x^2$와 $s_z^2$을 대각원소로 놓고, 변수들 사이의 공분산을 비대각원소에 위치시킨 행렬
        
        $\hat{\sum} = \begin{bmatrix}
        
          s_x^2 & s_{x, z} \\
          s_{z, x} & s_z^2
        \end{bmatrix}$
        
        - c.f. 변수를 z 점수로 변환할 때 표준편차 사용한 것처럼, 다변량분석에서 표준화처리를 하기 위해 공분산행렬 사용 (마할라노비스 거리. LDA 함수와 관련)

### 5.2.2 피셔의 선형판별

- 문제: 두 개의 연속형 변수 (x, z) → binary response y 예측
- 판별분석에서 X에 대한 가정
    - 엄밀히 말하면 X가 1) 정규분포를 따르는 2) 연속형 변수라는 가정이 있음
    - 실제로는 정규분포에서 벗어나거나 이진-연속형이 아닌- 변수에 대해서 잘 작동
- 그룹 안의 편차와 그룹 간의 편차 구분
- LDA(선형판별분석)
    - X를 두 그룹으로 나누는 방법
    - 내부 제곱합에 대한 사이 제곱합의 비율을 최대화 하는 것이 목표
        - 내부 제곱합: 그룹 안의 변동 측정
            - 공분산 행렬에 의해 가중치가 적용된 각 그룹 내의 평균을 주변으로 퍼져 있는 정도
        - 사이 제곱합: 그룹 사이의 편차를 측정
            - 두 그룹 평균 사이의 거리 제곱
        - 제곱합 비율 ($SS_{사이}\over SS_{내부}$)를 최대화 하는 선형 결합  $w^T X = w_x x + w_z z$ 를 찾는다
            - 판별함수 $\delta(x)$ 식을 정리하면 vector X에 대한 내적으로 표현 가능해진다
    - LDA 돌리기 전에 미리 예측변수들을 정규화했다면, Discriminator weight는 변수의 중요도를 의미하게 됨 → feature selection에 효과적
    - LDA 그래프
        
        ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%201.png)
        
        - 해석
            - X: 채무자의 신용 점수. Y: 소득에 대한 지급 비율
            - 파란색은 연체될 가능성이 낮고, 붉은 색은 연체될 가능성이 높음을 의미
            - 실선을 기준으로 연체할 것으로 예측되는 영역(붉은색), 아닌 영역(푸른색) 구분
            - 실선으로부터 멀 수록 점의 색이 진해지고, 가능성이 0.5로부터 멀어짐을 의미
                - 예측 결과에 대한 신뢰도가 높다
- 판별분석의 확장
    - 예측변수가 2개보다 많아지는 경우
        - 변수당 충분한 레코드가 있으면 공분산 계산 가능 → LDA 잘 동작
    - 이차판별분석 (QDA, Quadratic Discriminant Analysis)
        - LDA에서는 클래스들이 하나의 공분산 행렬을 공유하지만
        - QDA에서는 클래스별로 서로 다른 공분산을 가질 수 있다
        - 실무적으로는 대부분의 경우 차이가 크지 않다

## 5.3 로지스틱 회귀

- 결과가 이진형 변수라는 점만 빼면 다중선형회귀와 유사
- 데이터 위주라기보다는 구조화된 모델 접근 방식
- 빠른 계산 속도 및 새로운 데이터에 대한 빠른 점수 산정이 장점

### 5.3.1 로지스틱 response 함수와 로짓

- 확률을 선형 모델링에 적합한 더 확장된 단위로 매핑
- 결과변수를 이진 값이 아니라 라벨이 1이 될 확률 p로 생각
- p를 X들의 선형함수로 모델링하고 싶은 유혹 ($p = \beta_0 + \beta_1x_1 + ... + \beta_px_p$)
    - 이 모델을 피팅한다고 해도, 선형 모델이다보니 p가 0과 1 사이로 딱 떨어지지 않을 수 있음
    
    ⇒ X들(예측변수)에 logistic response, inverse logit 함수를 적용해서 p를 모델링 
    
- logistic response function
    
    $p = {1 \over {1 + e^{-(\beta_{0} + \beta_1x_1 + ... \beta_qx_q)}}}$
    
    - 이렇게 하면 p는 항상 0과 1 사이에 오게 됨
    - 분모의 지수 부분을 구하려면 확률 대신 오즈비(odds ratio)를 이용
        - 사건이 발생할 확률 / 사건이 발생하지 않은 확률
        - 오즈(Y=1) = ${p \over {1-p}}$
    - inverse odds ratio
        - $p = {오즈 \over {1+오즈}}$
    - 오즈 수식을 logistic response 함수에 적용하면
        - 오즈(Y=1) =  $e^{\beta_{0} + \beta_1x_1 + ... \beta_qx_q}$
    - 양변에 로그 함수를 취하면, 예측변수에 대한 선형 함수를 얻을 수 있다
        - log(오즈(Y=1)) = $\beta_0 + \beta_1x_1 + ... + \beta_px_p$
        
        = 로그 오즈 함수, 로짓 함수. 0~1 사이의 확률 p를 -+inf 값으로 맵핑 
        
        - 확률을 예측할 수 있는 선형 모형을 구함 → 절사 기준을 통해 그 값보다 큰 확률값이 나오면 1로 분류하는 식 → 클래스 라벨 구할 수 있다

### 5.3.2 로지스틱 회귀와 GLM

- 로그 오즈 함수의 결과는 binary가 아님 vs. 우리가 실제로 관찰한 데이터는 binary
    - 로지스틱 회귀 방정식을 피팅하기 위해 특별한 확률 기법 필요
    - 로지스틱 회귀는 선형회귀를 확장한 GLM(일반화선형모형)의 특별한 사례
- loan_data 설명
    - Y : outcome. 대출을 모두 갚았으면 0, 연체면 1
    - X: purpose_(대출 목적), home_(주택 소유 상태)
        - borrow_score: 차용인의 신용도. 0~1(불량~우수)
        - purpose_의 기준 수준: credit_card, home_의 기준 수준: MORTGAGE
- R에서의 glm-binomial
    - P개의 수준을 갖는 요인변수는 P-1개의 열로 표시 가능
    - 기준 코딩 사용. 기준 수준에 다른 수준들을 비교해서 사용

### 5.3.3 일반화선형모형

- Generalized Linear Model (GLM)
- 구성 요소
    1. 확률분포, 분포군 
        - 로지스틱 회귀의 경우 이항분포
        - 카운트 데이터 → 푸아송 분포
            - 사용자가 일정 시간 동안 웹페이지를 방문한 횟수
        - 경과 시간 → 음이항분포, 감마 분포
            - 고장 시간
    2. 응답을 예측변수에 맵핑하는 연결(변형 함수) 
        - 로지스틱 회귀의 경우 로짓

### 5.3.4 로지스틱 회귀의 예측값

- 로지스틱 회귀의 예측값: 로그 오즈에 관한 값
    - $\hat{Y} = log(오즈(Y=1))$
- 예측된 확률: 로그 오즈를 로지스틱 반응 함수에 넣어야
    - $\hat{p} = {1 \over {1 + e^{-\hat{Y}}}}$
- 왜 실습 코드에서 Y를 순서형 범주로 바꾸는가?
    
    ```r
    loan_data$outcome <- ordered(loan_data$outcome, levels=c('paid off', 'default')) 
    ```
    
    - 일반적인 이유
        - 결과 간의 순서 관계를 명확히 하여 해석을 돕기 위해
        - 특정 모델링 기법이나 알고리즘의 요구사항을 만족시키기 위해
            - 그러나 로지스틱 회귀에서 꼭 Y가 순서형이어야 하는 건 아니다
        - 데이터 전처리 과정에서 변수의 성격을 명시적으로 정의하기 위해
    - 이 상황에서의 이유
        - 어차피 binary 범주라 순서가 있던 없던 큰 상관이 없는 것 같은데 다만
        - ‘paid off’는 긍정적인 결과, ‘default’는 부정적인 결과로 해석될 수 있으며, 이를 순서형으로 해석함으로써 더 명확하게 분석할 수 있다고 하는데…(의심)
- c.f. R에서 predict(logistic_model)은 `로그 오즈` 를 반환하고, scikit-learn에서 logistic_model.predict_log_proba(X)는 `로그 확률` 을 반환
    - 진짜 클래스별 확률을 얻으려면 둘의 결과를 서로 다르게 변환해야 함. R 결과는 logistic response function 넣으면 되고, sklearn 결과는 `predict_proba` 라는 다른 method 쓰면 됨
- 확률만 가지고는 예측 결과를 분명히 알기 어려움
    - 0.5 같은 기준값이 있어야
- 최종정리
    
    ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%202.png)
    

### 5.3.5 계수와 오즈비 해석하기 (❤️‍🔥)

- Logistic Regression 장점
    - 재계산없이 새 데이터에 대해 결과 빨리 계산 가능
    - 다른 분류 방법들에 비해 모델 해석하기 상대적으로 쉬움
- Odds ratio 이해
    - binary 변수 X 가정
    - Odds ratio
        
        $오즈(Y=1 | X = 1) \over {오즈(Y=1 | X=0)}$
        
        - 이 비율이 2다 → X=0일 때 Y=1 일 ~~확률~~ 오즈보다 X=1 일때 Y=1 ~~확률~~오즈이 두배 높다
        - 오즈
            
             $\text{오즈}(Y=1 \mid X=1) = \frac{P(Y=1 \mid X=1)}{P(Y=0 \mid X=1)} = \frac{P(Y=1 \mid X=1)}{1 - P(Y=1 \mid X=1)}$ 
            
             $\text{오즈}(Y=1 \mid X=0) = \frac{P(Y=1 \mid X=0)}{P(Y=0 \mid X=0)} = \frac{P(Y=1 \mid X=0)}{1 - P(Y=1 \mid X=0)}$ 
            
- 왜 굳이 확률 대신 오즈비를 사용해 귀찮은 일을 할까?
    - 로지스틱 회귀분석에서 계수 $\beta_{j}$는 $X_{j}$에 대한 **오즈비의 로그값**이기 때문에
    - 범주형 X 변수 예시
        - purpose_small_business 변수에 대한 회귀계수가 1.21526인 경우
            - 기준 요인: purpose_credit_card
                
                ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%203.png)
                
                - 이 때 $\beta_0$ 는
                    
                    ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%204.png)
                    
            - 신용카드 빚 상환 대출과 비교했을 때, 소규모 사업을 위한 대출은 exp(1.21526) ≈ 3.4만큼 대출 상환 대비 연체의 오즈비가 증가한다는 것을 의미
            - 절편 $\beta_0$: 독립 변수들이 모두 0일 때의 **로그 오즈**
            - 계수 $\beta_j$: 해당 독립 변수 $X_j$가 1 단위 증가할 때 로그 오즈의 변화량을 나타내며, 이는 **오즈비의 로그**입니다.
    - 수치형 X 변수 예시
        - X에서 단위 크기만큼 변화할 때 오즈비의 변화
        - payment_inc_ratio: payment / income 비율
            - 5→6만큼 증가했다고 하면 exp(0.07974)  ≈ 1.08만큼 연체할 오즈비가 증가 (책에는 0.08244로 나와 있는데 why?)
        - borrower_score: 대출자의 신용도. 0~1 사이
            - 현재 연체 중인 최악의 차용인(0)에 대한 가장 우수한 차용인의 오즈비는 exp(-4.61264) ≈ 0.01 정도로 훨씬 더 적다
            - 가장 신용이 불량한 차용인의 연체 위험도는 신용이 가장 좋은 차용자에 비해 100 정도라는 의미
    - 헷갈리는 로지스틱 회귀 결과 분석
        - 예) y=0는 paid_off, y=1는 default. X는 대출 목적. 기준 요인은 credit_card
            - 로지스틱 회귀 모델 계수
                
                ```
                •	Intercept(절편): -1.5
                •	purpose_car: 0.8
                •	purpose_home_improvement: 0.5
                •	purpose_vacation: 1.2
                ```
                
            - Intercept 해석
                - 기준 요인(credit_card)일 때 로그 오즈가 -1.5임을 의미합니다.
                    - credit_card 목적으로 대출을 받을 때 default(y=1)할 로그 오즈는 -1.5
                        
                        ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%205.png)
                        
                        - 오즈로 변환하면, $e^{-1.5} \approx 0.223$
            - purpose_car 해석
                - credit_card에 비해 car 목적으로 대출을 받을 때 default할 로그 오즈가 0.8만큼 더 큼을 의미합니다.
                    - 오즈 비로 변환하면 $e^{0.8} \approx 2.23$
            
            ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%206.png)
            
    - 최종 정리
        - logistic_model로 요인/변수별 계수를 얻는다
        - 계수: 기준 요인보다 해당 요인일 때 Y = 1 (연체)일 로그 오즈비가 몇 배인지
        - 로그는 해석하기 어려우니까 e^(로그 오즈비)를 해서 오즈비를 얻는다
        - 확률을 얻고 싶으면 ${1 \over {(1 + e^{-로짓})}} = {e^{로짓} \over {(e^{로짓} + 1 )}}$
            - 이 확률은 P(Y=1|X=x)
            - P(Y=0|X=x)를 얻고 싶으면 1 - P(Y=1|X=x) 하면 됨
    - 추가 자료 - 수치에 대한 맥락을 부여한다는 관점에서 확률과 오즈
        
        ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%207.png)
        

### 5.3.6 선형회귀와 로지스틱 회귀: 유사점과 차이점

- 공통점
    - X, Y를 선형 관계로 가정
    - 가장 좋은 모델을 탐색하고 찾는 과정도 유사
        - 예측변수에 스플라인 변환 사용하는 방법을 로지스틱 회귀 설정에서도 똑같이 적용 가능
- 차이점
    1. 모델을 피팅하는 방식 (최소 제곱 사용 불가)
    2. 모델에서의 잔차의 특징과 분석
- 모델 피팅
    - 선형회귀
        - 모델 피팅 : 최소 제곱, 피팅 성능 평가: RMSE, R^2 통계량
        - c.f. 닫힌 형태의 해 존재
            
            ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%208.png)
            
            - 선대 복습
                - 편미분을 0으로 설정하는 이유는, 함수의 극값(최소값 또는 최대값)은 미분값이 0이 되는 지점에서 발생
                - 행렬 A가 가역적이려면 A는 정사각행렬이어야 하고, 행렬식(determinant)이 0이 아니어야
                    - 행렬식  = ad-bc
    - 로지스틱 회귀
        - 모델 피팅 닫힌 형태의 해가 없으므로 최대우도추정(Maximum Likelihood Estimation, MLE) 사용
        - MLE
            - 우리가 보고 있는 데이터를 생성했을 가능성이 가장 큰 모델을 찾는 프로세스
            - 로지스틱 회귀식에서 Y(응답변수)는 0이나 1이 아니라, 응답이 1인 로그 오즈(로짓)의 추정치
                - 예상 로그 오즈가 관찰된 결과를 가장 잘 설명하는 모델을 탐색
            - 알고리즘 - quasi Newton optimization
                - 현재 파라미터에 기반하여 점수를 얻는 단계(Fisher’s scoring)와 적합성을 향상시키는 방향으로 파라미터를 업데이트 하는 단계를 왔다갔다 반복
    - 최대우도추정
        - 일련의 데이터 $X_1, X_2, ..., X_n$
        - 파라미터 집합 $\theta$에 따른 확률모형 $P_{\theta}(X_1, X_2, ..., X_n)$
        - MLE 목표: $P_{\theta}(X_1, X_2, ..., X_n)$ 값을 최대화하는 $\hat{\theta}$를 찾는 것
            - 주어진 모델 P에서 $X_1, X_2, ..., X_n$를 관측할 확률을 최대화
        - 피팅 과정에서 편차라는 지표를 사용하여 모델 평가
            - 편차 = $-2log(P_{\hat{\theta}}(X_1, X_2, ..., X_n))$
                - -2를 곱하는 이유
                    - 통계적으로 유의미한 분포(카이제곱분포)를 따르기 때문에, 모델 적합도를 평가하는 데 유용
                    - 유의수준에서의 판단
                        - 계산된 편차 통계량 D를  카이제곱분포의 임계값과 비교
                        - 만약 D가 임계값보다 크다면, 모델이 데이터를 잘 설명하지 못한다고 판단
            - 편차가 작을 수록 모델 적합도가 높은 것을 의미
- 요인변수 다루기
    - 로지스틱 회귀에서 요인변수는 선형회귀처럼 인코딩 과정을 거쳐야 함
    - R에서는 기준 인코딩, sklearn 에서는 원핫인코딩 (결과 더비 변수에서 n-1개만 회귀에 사용)

### 5.3.7 모델 평가하기

- 평가 기준
    - 모델이 새로운 데이터를 얼마나 정확하게 분류하는가
    - 선형회귀에서처럼 표준 통계 도구들을 사용해 모델 시험, 향상 가능
    - R은 계수들의 표준오차(SE), z점수, p값 출력
        1. Std. Error (표준 오차)
            - 계수 추정치의 표준 오차. 이는 추정치의 불확실성을 나타내.
            - 표준 오차가 작을수록 계수 추정치가 더 정확하다는 의미. 그러나 절대적인 숫자인 이 값만 보고는 어떤 결론을 내리기 어려움
        2. z value (z 값)
            - 계수 추정치를 표준 오차로 나눈 값 (Estimate / SE)
            - 표준 정규 분포를 따르는 값으로, t 검정의 t 값과 유사한 역할
            - z 값이 크면 해당 계수 추정치가 통계적으로 유의미하다는 것
                - 이는 귀무가설(해당 계수가 0이다)을 기각할 수 있는지를 판단하는 데 사용
        3. Pr(>|z|) (유의확률)
            - z 값의 절대값이 주어진 표준 정규 분포에서 관찰될 확률
            - p-값으로, 귀무가설을 기각할 수 있는지를 나타내는 지표야
                - p-값이 작을수록 (일반적으로 0.05보다 작으면) 해당 독립 변수의 계수가 0이 아니라는 귀무가설을 기각. 즉, 해당 독립 변수가 종속 변수에 유의미한 영향을 미친다는 것을 의미
            - c.f. 근데 책에서는 이걸 통계적 유의성을 측정하는 지표가 아니라, 변수의 중요성을 나타내는 상대적인 지표로 보라고 함
        - z값이랑 p값이랑 왜 둘다 보여주는지
            1. **직관적인 이해**:
                - z 값: 계수 추정치가 표준 오차와 비교해 얼마나 큰지 직관적으로 보여줌
                - p 값: 귀무가설 하에서 해당 계수가 우연히 나타날 확률
                    - 유의수준과 비교해 통계적 유의성을 쉽게 판단
            2. **통계적 해석의 용이성**:
                - p값은 연구자들이나 분석가들이 특정 임계값(예: p < 0.05)과 비교하여 결과의 유의성을 판단
                - z 값은 효과의 크기(절댓값)와 방향(부호)을 더 직관적으로 이해하는 데 도움
            3. **다양한 활용**:
                - z 값은 효과의 크기를 이해하는 데 유용. 표준 오차와 비교해 상대적인 크기를 직관적으로 알 수 있음
                    - 효과의 크기 예시
                        - 예를 들어, 두 개의 계수가 각각 2.5와 0.5
                        - 이들의 표준 오차가 각각 0.5와 0.1이라면, z 값은 각각 5와 5로 동일
                        - 이는 두 계수가 각각의 맥락에서 유사한 정도로 통계적으로 유의미함을 나타내.
                - p 값은 통계적 검정에서의 유의성 판단을 용이하게 함. 예를 들어, 여러 계수에 대한 유의성을 빠르게 비교 가능
    - 로지스틱 회귀는 binary Y라서 RMSE나 R**2가 있을 수 없음
        - 왜 이진 변수(0/1) - 확률(예측 결과) 간의 차이는 중요하게 고려되지 않는가?
            
            ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%209.png)
            
            - 0.5가 경계라고 하면, Predicted Label = [0, 1, 0, 1, 0, 1]
                - 일반 분류 지표로 따지면 모두 정답을 맞춘 셈
            - RMSE로 계산하면 0.2646 → 이거 자체로 어떤 의미가 있다고 하기 어려움
                1. Compute the squared error for each prediction:
                    
                    Sample 1: (0 - 0.1)² = 0.01
                    
                    Sample 2: (1 - 0.9)² = 0.01
                    
                    Sample 3: (0 - 0.4)² = 0.16
                    
                    Sample 4: (1 - 0.6)² = 0.16
                    
                    Sample 5: (0 - 0.2)² = 0.04
                    
                    Sample 6: (1 - 0.8)² = 0.04
                    
                2. Sum the squared errors: 0.01 + 0.01 + 0.16 + 0.16 + 0.04 + 0.04 = 0.42
                3. Divide by the number of samples (6) and take the square root:
                    - RMSE = sqrt(0.42 / 6) ≈ 0.2646
            - 두 모델을 비교할 수 있을 것처럼 보이지만, Predicted Label이 동일한 두 모델도 확률에 따라 RMSE가 다르게 나올 수 있는 반례도 있음
- 선형 회귀에서 적용되었던 개념들이 로지스틱 회귀나 다른 GLM에도 똑같이 이어짐
    - 단계적 회귀, 상호작용 항 도입, 스플라인 항 포함 등
    - 교란변수나 변수 상관과 관련된 문제들도 동일하게 고려해야 함
    - c.f. GAM은 로지스틱 회귀의 유연한 확장 형태
        - 비선형 관계를 포함할 수 있는 강력한 모델링 기법
        - 로그 오즈를 만드는 식이 비선형이 된다는 듯?
- 잔차분석
    - 로지스틱 회귀와 선형회귀가 다른 부분
    - 편잔차 그림 해석
        
        ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%2010.png)
        
        - 빨간선: 회귀선
        - 결과 변수가 이진형이라 로지스틱 회귀에서 얻은 잔차는 보통 이러한 형태
            - 위쪽 구름: 1의 응답(연체)
            - 아래쪽 구름: 0의 응답(대출 상환)
        - 잔차는 어떠한 경우에도 0이 될 수 없음
            - 예측 결과는 항상 유한한 값인 로짓
            - 실제값(0/1)은 양수 혹은 음수 무한대인 로짓에 해당
                - 실제값이 0인 경우, 사건이 발생할 확률 p 는 0에 가까워짐
                    - 오즈는$\frac{0}{1-0} = 0$ 이 되고, $\log(0)$는 음의 무한대(-∞)
                - 실제값이 1인 경우, 사건이 발생할 확률 p 는 1에 가까워짐
                    - 오즈는 $\frac{1}{1-1} = \infty$이 되고, $\log(\infty)$는 양의 무한대(∞)
            
            → 편잔차 그래프에서 점들이 fitting line 위와 아래에 구름처럼 퍼져 있음 
            
        - [ ]  왜 python에서는 구하는 방법이 다른지 모르겠음. 특히 편잔차 계산 방법
    - 회귀에서보다 덜 중요하긴 하지만, 비선형성을 검증하고 영향력이 큰 레코드들을 확인하는데 유용
        - LR에서 잔차는 확률과 이진 값의 차이이므로, 직접적인 모델 성능 평가에 덜 중요
- c.f. summary 함수 출력 내용
    - 로지스틱 회귀는 최대 우도 추정 사용하여 모델 피팅
        - 잔차 편차나 점수화 반복 횟수는 최대 우도 피팅과 관련
            - `Residual deviance: 57515  on 45330  degrees of freedom`
            - `Number of Fisher Scoring iterations: 4`
    - 분포도 파라미터는 다른 유형의 GLM에는 적용되지만, 로지스틱 회귀에는 적용 X
        - 분포도 파라미터와 관련된 출력
            - **Residuals**
                - 잔차의 최소값, 1사분위수(Q1), 중앙값, 3사분위수(Q3), 최대값
                - 잔차의 분포를 요약하여 모델이 얼마나 잘 적합되는지
            - **Residual standard error**
                - 잔차의 표준 오차
                - 잔차의 분산을 측정하며, 잔차의 표준 오차가 작을수록 모델이 데이터를 잘 설명한다는 의미
            - **F-statistic**
                - 모델의 전체 유의성을 평가
                - 분산 분석(ANOVA)을 통해 계산
                - 모델이 무작위 추정보다 데이터를 더 잘 설명하는지 여부를 나타냄
                    - p-값이 작을수록 모델이 유의하다는 것을 의미

## 5.4 분류모델 평가하기

### 5.4.0 개요

- 타당성 검사 및 검증
    - 각 모델에 홀드아웃 표본 적용
    - 모델 평가, 튜닝하고도 데이터가 충분하면, 이전에 사용되지 않은 (training, validation 어디에도) 별도의 홀드아웃 샘플을 사용하여 완전히 새로운 데이터에 대해 어떤 성능 보이는지 추정
- 정확도
    - (# of 참 양성 + # of 참 음성) / 표본 크기
- 컷 오프
    - 확률 예측 값을 기반으로 특정 클래스에 할당할지 여부를 결정하는 데 사용
    - 대부분 분류 알고리즘에서는 각 데이터에 대해 1이 될 확률값을 추정하여 할당
        - 모든 방법이 비편향 확률 예측을 하는 것은 아니다
            - 비편향 확률 예측: 예측된 확률 값이 실제 클래스 분포와 일치하는 것을 의미
                - 예를 들어, 100개의 샘플 중 70개가 실제로 클래스 1이라면, 비편향 확률 예측은 이 샘플들에 대해 평균적으로 70%의 확률을 예측
        - 대부분의 분류 알고리즘은 실제로 비편향 확률을 예측하기보다는 각 데이터 포인트에 대해 상대적인 순위를 제공
            - 확률 예측 값이 절대적으로 정확하지 않더라도, 어떤 샘플이 다른 샘플보다 더 높은 확률을 갖는지에 대한 순위 정보는 신뢰할 수 있음
    - 가장 기본적인 컷 오프 기준값은 0.5
        - 확률이 0.5보다 크면 분류 결과는 1, 그렇지 않으면 0
    - 실제 데이터에서 1이 차지하는 비율을 컷오프로 사용하는 방법도 있음

### 5.4.1 혼동 행렬

- 분류 결과를 나태는 가장 대표적인 행렬
    - 응답 유형별로 정확한 예측과 잘못된 예측의 수를 한 번에 보여주는 표
- c.f. 0.5 컷 오프 기준이면 - 확률 p가 0.5일 때: 오즈 p/1-p = 1이 되고, 로그 오즈 log(1) = 0
- 혼동 행렬
    
    ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%2011.png)
    
    - 행: 실제 결과 X 열: 예측값. preds
    - 대각원소:정확히 예측한 데이터의 수 (TP, TN)
    - 비대각원소: 부정확한 예측의 수 (FP, FN)
    - false_XX: pred 결과가 XX로 나오고, true는 반대로 나온 경우
    - false positive ratio
        - 눈에 잘 안 띄지만 중요한 지표 중 하나
        - 결과가 1인 데이터의수가 희박할 때, 모든 y_hat에 대해 false positive 값의 비율이 높아지는 경우가 있음
            - 예측 결과는 1이지만, 실제로는 0일 가능성이 높은 상황
            - 의료 검진 검사(유방 조영술)을 어렵게 하는 요인 중 하나

### 5.4.2 희귀 클래스 문제

- 분류해야 할 클래스 간에 불균형이 존재하는경우
    - 예) 합법적 vs. 사기성 보험청구, 웹사이트 단순 방문객 vs. 실구매자
- 더 희귀하고 중요한 클래스를 1로 설정하는 게일반적
- 클래스를 쉽게 분리하기 어려운 경우 - 가장 정확도가 높은 분류 모델은 모든 것을 무조건 0으로 분류 하는 모델일 수도
    - 예) 인터넷 쇼핑몰의 방문객 중 0.1%만이 실제 구매
        - ‘모든 방문객이 구매를 하지 않을 것이다’라고 예측하는 모델의 정확도는 99.9%
        - 그러나 이런 모델은 있으나 마나 한 모델
        - 비구매자를 잘못 구분해서 전반적인 정확도가 떨어지더라도, 실제 구매자를 잘 골라내는 모델이 선호됨

### 5.4.3 정밀도, 재현율, 특이도

<aside>
<img src="https://www.notion.so/icons/checkmark_gray.svg" alt="https://www.notion.so/icons/checkmark_gray.svg" width="40px" /> 정밀도는 “정확히 맞추는 것”에 집중하고, 재현율은 “놓치지 않는 것”에 집중한다.

</aside>

- 정밀도(Precision): 예측된 양성 결과의 정확도
    - # of 참 양성 / (# of 참 양성 + # of 거짓 양성)
    - 양성으로 분류된 모든 예측 결과 수 중에 진짜 양성의 비율
- 재현율(민감도, Recall): 양성 데이터에 대해 정확히 1이라고 예측하는 결과의 비율
    - # of 참 양성/ (# of 참 양성 + # of 거짓 음성)
    - 원래 양성인 모든 데이터 수 중에 양성으로 분류 결과도 나온 비율
- 특이도(Specificity): 음성 결과를 정확히 예측하는 능력을 측정
    - # of 참 음성 / (# of 참 음성 + # of 거짓 양성)
    - 원래 음성인 모든 데이터 수 중에 음성으로 분류 결과도 나온 비율
    
    c.f. Specificity = 1 - FPR
    
    - FPR: 음성 샘플 중에서 잘못 양성으로 예측된 비율.
    - Specificity: 음성 샘플 중에서 올바르게 음성으로 예측된 비율.

### 5.4.4 ROC 곡선

- recall과 specificity 사이에는 trade-off 관계
    - 1을 잘 잡아낸다 = 0을 1로 잘못 예측할 가능성도 높아짐
- trade-off 관계를 표현하기 위한 지표가 ROC(receiver operating characteristic) 곡선
    - x축의 specificity vs. y축의 recall
- 레코드를 분류할 때 사용하는 컷오프 값을 바꾸면 recall, specificity 관계가 어떻게 변하는지 잘 보여줌
- y축에 recall을 표시하면서 x축에는 두 가지 형태로 표시
    - x축 왼쪽에 1부터 오른쪽 0까지 특이도 표시
    - x축 왼쪽에 0부터 오른쪽 1까지 (1-특이도)를 표시
    - 두 방법 중 어떤 걸 써도 곡선의 모양은 동일
- ROC 곡선 계산 과정
    1. 1로 예측할 확률에 따라 가장 1이 되기 쉬운 것부터 1이 되기 어려운 순으로 레코드를 정렬 (logit내림차순)
    2. 정렬된 순서대로 점증적으로 특이도와 재현율을 계산
        - 책에 안나와 있는 실제 계산 과정 ❤️‍🔥
            
            `recall <- cumsum(true_y[idx] == 1) / sum(true_y == 1)`
            
            - idx에서 pred 수치가 없는데 어떻게 참 양성의 개수가 구해지는가?
                - pred값 자체가 임계점이 되기 때문
                    - 이번 idx 앞에 있는 모든 원소는 1로 predict. (y_hat = 1)
                    - 여기서 참 양성의 개수는 원래 y 값이 1인 것의 개수이므로 (y = 1), 그 개수를 누적하기만 하면 됨
                        - 참 양성은 예측 값이 1인 것과 실제 값이 1인 것의 교집합
                    - 예
                        - 예측값 (pred): [0.9, 0.8, 0.4, 0.6, 0.2], 실제값 (true_y): [1, 0, 1, 0, 1]
                        1. 예측값을 내림차순으로 정렬
                            - 정렬된 예측값의 인덱스 (idx): [1, 2, 4, 3, 5]
                                - 정렬된 예측값 (pred[idx]): [0.9, 0.8, 0.6, 0.4, 0.2]
                                - 정렬된 실제값 (true_y[idx]): [1, 0, 0, 1, 1]
                        2. 누적 참값의 수 계산
                            - 0.9가 threshold라고 하면, 0.9만 양성 예측이고, 나머지는 음성 예측
                                - 실제 양성 값도 1개라서 누적 참 양성의 개수는 1
                            - 0.8이 threshold라고 하면, 0.9, 0.8까지만 양성 예측이고, 나머지는 음성 예측
                                - 0.8에서 실제 양성 값은 0개라서 누적 참 양성 개수는 그대로 1
                            - 0.4가 threshold라고 하면, 0.9~0.4까지 총 3개가 양성 예측인데, 여기서 실제 값이 양성인 개수(=참 양성의 개수)는 2
                            - 0.2가 threshold라고 하면, 0.9~0.2까지 총 5개가 양성 예측. 여기서 실제 데이터도 양성인 개수는 총 3개
            
            `specificity ← (sum(true_y == 0) - cumsum(true_y[idx] == 0)) / sum(true_y == 0)`
            
            - `(sum(true_y == 0) - cumsum(true_y[idx] == 0))`
                - 참 양성은 recall이랑 같은 방식으로 하면 cumsum(true_y[idx]==0)에서 끝날 것 같지만, 특이도가 들어가는 x축은 왼쪽이 1, 오른쪽이 0으로 들어감
                    - 예
                        - 예측값 (pred): [0.9, 0.8, 0.4, 0.6, 0.2], 실제값 (true_y): [1, 0, 1, 0, 1]
                        1. 예측값을 내림차순으로 정렬
                            - 정렬된 예측값의 인덱스 (idx): [1, 2, 4, 3, 5]
                                - 정렬된 예측값 (pred[idx]): [0.9, 0.8, 0.6, 0.4, 0.2]
                                - 정렬된 실제값 (true_y[idx]): [1, 0, 0, 1, 1]
                        2. 누적 참 음성 수 계산
                            - 0.9가 threshold라고 하면, 0.9만 양성 예측이고, 나머지는 음성 예측
                                - 정렬된 실제값에서 제일 처음 값은 양성 예측이라 제외하고
                                - 나머지 4개 값에서 실제 음성 값은 2개
                                
                                → 참 음성 개수 2
                                
                            - 0.8이 threshold라고 하면, 0.9, 0.8까지만 양성 예측이고, 나머지는 음성 예측
                                - 정렬된 실제값에서 앞에 두 개는 양성 예측이라 제외하고
                                - 나머지 3개 값에서 실제 음성 값은 1개
                                
                                → 참 음성 개수 1 
                                
                            - 0.4가 threshold라고 하면, 0.9~0.4까지 총 4개가 양성 예측
                                - 앞의 3개 제외하고 실제 값에서 음성인 개수는 0개
                            - 0.2가 threshold라고 하면, 0.9~0.2까지 총 5개가 양성 예측
                                - 참 음성의 개수는 0
                        
                - `cumsum(true_y[idx] == 0)`
                    - 현재 임계 값은 pred[idx] 라서 idx까지는 모두 양성으로 예측
                        - 참 음성에서 제외되어야 함
                        - 실제 label이 양성인 것의 개수는 어차피 sum(true_y == 0)에 포함되지 않으므로 상관 없음
                        - 실제 label이 음성이면서, 예측 label이 양성인 false positive를 제외해야 함
                        - idx까지 예측 label은 모두 양성이므로, 실제 label이 음성인 것의 누적 개수만 구하면 됨
                    - 실제 label은 음성이면서 예측만 양성인 false positive의 누적 개수를 의미
- ROC 곡선 해석
    - 그림
        
        ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%2012.png)
        
    - 점선: 랜덤으로 예측했을 때의 결과
    - 극단적으로 효과적인 분류기는 ROC 곡선(의 볼록한 부분이)이 왼쪽 상단(모서리)에 가까운 형태를 보일 것
        - 왼쪽 상단이 recall도 1이고 specificity도 1이기 때문에
    - 적어도 50% 정도의 specificity를 원한다면 recall은 75% 정도가 될 것
- Precision-Recall 곡선을 사용하기도 함
    - 클래스 간 데이터 불균형이 심할 때 특히 유용
        - 음성 클래스가 많은 경우, False Positive Rate가 낮게 나옴. Specificity가 1 - FPR이라 높게 나오기 때문에 ROC 곡선이 모델을 과대평가할 수 있다고 함
        - Precision은 예측 값이 양성인 것 중에 실제로 양성인 비율이라서, 음성 클래스보다 양성 클래스에 대한 모델의 성능을 알 수 있으므로 유용
    1. 확률이 낮은 경우에서 높은 경우로 데이터를 정렬(오름차순)
    2. 차례대로 정밀도, 재현율 계산

### 5.4.5 AUC 곡선

- ROC 곡선을 이용해 곡선 아래 면적 지표 계산: AUC (area underneath the curve)
    - ROC 곡선의 아래쪽 면적이 클 수록 좋은 classifier
    - 가장 좋은 값은 1. 0을 1로 잘못 예측하는 경우 없이(specificity=1) 1을 정확히 분류(recall=1)
    - 가장 나쁜 값은 0.5. ROC 곡선이 가운데를 지나가는 직선인 경우
        - 어떤 임계값에서든 specificity, recall이 같다는 의미
        - 참 양성을 찾아내는 확률과 거짓 양성을 만들어내는 확률이 동일 = 무작위 추측
    - R에서 적분 이용
        
        `sum(roc_df$recall[-1] * diff(1 - roc_df$specificity))`
        
        - `recall[-1]` : recall 칼럼의 첫번째 요소를 제외하고 나머지 값들
            - 아래의 행간 차이에서 첫번째 행에 대해서는 행 간 차이가 존재하지 않기 때문에
        - `diff(1 - roc_df$specificity)`
            - 1-specificity: false positive rate
            - diff: 주어진 벡터에서 인접한 값들의 차이를 계산하는 함수
        - 구간별 y 값과 변화량을 곱한 뒤 다 더해서 전체 영역을 구하는 식
    - [x]  python 코드에서는 recall[:-1] 인데, r에서는 recall[1:]로 사용;; → chatGPT에서는 recall[:-1]가 맞다고 함.

### 5.4.6 리프트

- 희귀 케이스 문제
    - 모든 레코드를 0으로 분류하지 않도록 하려면 컷오프를 0.5 미만으로 낮춰야 함
    - 1의 중요성을 너무 크게 반영하여 1을 과대평가하는 결과 낳을 수 있음
    - 컷 오프를 낮추면 0을 1로 잘못 분류하는 경우가 발생하지만 1을 포착할 가능성이 높아짐
- 상위 10%까지를 1로 분류하는 알고리즘이, 랜덤 선택 경우와 비교할 때 얼마나 더 나은가?
    - 1로 예측될 확률이 있는 레코드들을 정렬
    - 무작위일때 0.1%의 정확도였고, 상위 10%에서는 정확도가 0.3%라고 하면
    - 상위 10%에서 3의 리프트(gain, 이득)을 갖는다고 표현
- 매 십분위수마다 혹은 데이터 범위에서 연속적인 값을 따라 리프트 값을 얻을 수 있음
- 리프트 차트 계산
    - 누적 이득 차트(cumulative gains chart)를 먼저 작성해야 함
        - Cumulative Gains Chart
            
            ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%2013.png)
            
        - y축에 재현율, x축 총 레코드 수
        - Lift chart
            
            ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%2014.png)
            
            - cumulative gain chart에서 상위 10%만 1로 분류할 때 재현율이 base line 대비 3배 높았음 → 해당 x 값에서 lift chart의 y값 lift는 3
- 리프트 곡선
    - 임의 선택을 의미하는 대각선에 대한 누적 이득의 비율
    - 레코드를 1로 분류하기 위한 확률 컷오프 값에 따른 결과의 변화를 한눈에 볼 수 있게 해줌
        - 적합한 컷오프 값을 결정하기 위한 중간 단계
- 업리프트(uplift)
    - 리프트보다 더 제한적인 상황
    - A/B 검정을 수행하고, 처리 A나 B 가운데 하나를 예측변수로 사용하는 예측 모델에서 사용
        
        ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%2015.png)
        
    - 처리 A와 처리 B 사이의 개별적인 한 케이스에 대해 예측된 결과의 향상을 의미한다
        - 각 고객에 대해 두 가지 처리(A와 B)가 미치는 영향을 개별적으로 평가 → 개인화 서비스, 고객별 맞춤 전략

## 5.5 불균형 데이터 다루기

- 불균형 데이터에서 예측 모델링 성능을 향상할 몇 가지 방법

### 5.5.1 과소표본 추출

- 데이터의 개수가 충분할 때, 다수 클래스에서 적게 뽑아서 데이터 개수 균형 맞추기
    - 소수 클래스의 데이터가 수만개 정도 있으면 충분한 양
- [ ]  c.f. 실습에서 pandas-full_train_set 사용한 LR model output이 차이남. 코드 문제는 아니고 패키지 버전 문제로 추정됨 (관련 깃허브 이슈 제기도 있음: https://github.com/gedeck/practical-statistics-for-data-scientists/issues/34)
- 빚을 값는 (paid_off)의 데이터가 더 많기 때문에, 이러한 데이터로 학습한 모델의 예측 결과에서도 상환 쪽이 더 크게 나온다
- 그니까 결국 full_train_set에서 undersampling 해서 paid_off, default의 비율을 맞춘 것이 loan_data라는 말씀

### 5.5.2 과잉표본추출과 상향/하향 가중치

- undersampling 약점: 데이터의 일부가 버려지기 때문에 모든 정보를 활용하지 못한다는 점
- 상대적으로 작은 데이터 집합의 경우 복원 추출 방식(부트스트래핑)으로 희소 클래스 데이터를 upsampling
- 데이터에 가중치를 적용하는 방식으로 위와 같은 효과를 얻을 수 있음
    - 희소 클래스에 속하는 레코드 가중치를 높이고, 다수 클래스의 가중치를 낮추는 식으로
- c.f. glm-family = ‘quasibinomial’
    - 과대분산 처리
        - 모델의 예측 분산보다 데이터의 변동성이 더 클 때
        - 잔차 분석
            - 잔차의 분산을 보는 이유는 모델이 데이터의 변동성을 얼마나 잘 설명하는지를 평가하기 위해서입니다. 데이터를 설명하는 모델의 성능을 평가하는 가장 직접적인 방법 중 하나가 잔차 분석이기 때문입니다. 잔차는 모델이 예측한 값과 실제 값 간의 차이이므로, 잔차의 분산을 통해 모델이 데이터의 변동성을 얼마나 잘 설명하고 있는지를 확인할 수 있습니다.
            - 모델의 적합성 평가
                - 잔차는 모델이 설명하지 못한 부분이므로, 잔차의 분산을 통해 모델이 얼마나 데이터를 잘 설명하고 있는지 평가할 수 있습니다.
                - 잔차의 분산이 크면 모델이 데이터를 잘 설명하지 못하고, 과대분산 문제가 있을 가능성이 높습니다.
            - 모델 가정 검증
                - 이항 회귀 모델에서는 각 관측값의 분산이  \\mu (1 - \\mu) 로 가정됩니다. 잔차의 분산을 통해 이 가정이 타당한지 검증할 수 있습니다.
                - 만약 잔차의 분산이  \\mu (1 - \\mu) 보다 크다면, 과대분산이 존재한다고 판단할 수 있습니다.
            - 모델 비교 및 선택
                - 잔차의 분산을 통해 여러 모델을 비교하고, 과대분산을 잘 처리하는 모델을 선택할 수 있습니다.
                - 예를 들어, 표준 이항 모델과 quasibinomial 모델의 잔차 분산을 비교하여, 어떤 모델이 더 나은지 평가할 수 있습니다.
            
        - 코드
            
            ```python
            # 모델 적합
            model <- glm(outcome ~ predictors, data = data, family = binomial)
            
            # Pearson 잔차 계산
            pearson_residuals <- residuals(model, type = "pearson")
            pearson_chi2 <- sum(pearson_residuals^2)
            pearson_ratio <- pearson_chi2 / model$df.residual
            print(pearson_ratio)  # 1보다 크면 과대분산 가능성
            
            # Deviance 잔차 계산
            deviance_ratio <- model$deviance / model$df.residual
            print(deviance_ratio)  # 1보다 크면 과대분산 가능성
            ```
            
        - [ ]  model$df.residual
        - [ ]  model$deviance

### 5.5.3 데이터 생성

- 기존에 존재하는 데이터를 살짝 바꿔 새로운 레코드 만들기
- context
    - 데이터 개수가 제한적 → 알고리즘을 통해 분류 규칙을 세우기에 정보가 부족
    - 비슷하지만 기존의 데이터와 다른 데이터를 생성 → 조금 더 로버스트한 분류 규칙을 배울 수 있는 기회 주기
- SMOTE 알고리즘
    - synthetic minority oversampling technique
    - 업샘플링된(할?) 레코드와 비슷한 레코드 찾기
    - 원래 레코드와 이웃 레코드의 랜덤 가중 평균으로 새로운 합성 레코드 만들기
    - 각각의 예측변수(칼럼)에 대해 개별 가중치 생성
- c.f. SMOTE 구현 패키지들(실습 코드에는 사용할 수 없다고 나옴)
    - R: unbalanced-Racing 알고리즘 등. FNN 패키지 사용하여 직접 구현
    - 파이썬: imbalanced-learn → sklearn과 호환되는 API 사용하여 다양한 메서드 구현
    

### 5.5.4 비용 기반 분류

- 예-신규 대출
    - C: 연체로 인해 발생할 수 있는 비용
    - R: 대출 상환을 통해 얻을 수 있는 수익
    - 기대수익 = P(Y=0) * R + P(Y=1)* C
        - Y=1이 default라서 연체
        - 비용 + 수익 = 순이익
- 대출 결과를 단순히 연체나 상환 둘 중의 하나로 결정하는 대신, 대출을 통해 얻을 수 있는 기대 수익이 있는지 없는지로 결정
    - 가치가 적은 대출보다는 연체 확률이 더 높더라도 가치가 큰 대출을 선호하는 결정

### 5.5.5 예측 결과 분석

- 서로 다른 모델에 대한 결정 규칙 시각화
    - X_1: borrower_score, X_2: payment_inc_ratio → Y: outcome
    - LDA, 로지스틱 선형회귀는 거의 비슷한 결과
    - 트리는 계단 형태의 가장 이상한 결과
    - GAM을 이용한 로지스틱 회귀 모형이 트리 모델과 다른 선형 모델들을 서로 타협하는 결과
- 그래프
    
    ![Untitled](5%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%20da9dad8960d745bea168abf371355736/Untitled%2016.png)
    
    - 해석
        - 경계선을 기준으로 위쪽에 위치한 점들은 borrower_score가 낮고, payment_inc_ratio가 높음
            - payment_income_ratio
                - 매월 상환해야 하는 대출 상환액을 신청자의 월 소득으로 나눈 값
                - 높을 수록 월 소득 대비 많이 빌렸다는 것
            
            → 신용 점수는 낮고, 소득 대비 많이 빌렸다는 것이므로 default로 분류되는 부분 
            
        - 식에서 왜 borrower_score를 고정시키는 게 아니라, payment_inc_ratio를 고정시키고 borrower_score를 그에 맞춰서 구하는가?
            - 모름. 근데 식이 나오면 어느 쪽을 고정 시키든 다른 한쪽을 구할 수 있어서 상관없을듯?
            - gam + LR의 경우, 비선형함수라서 바로 구하기 어떤 계수를 가지고 주어진 y를 만드는 x를 찾기 어려움
                - [ ]  R코드에서 아래 함수를 최적화하는 X를 따로 구하는 이유를 모르겠다
                    - 특히 RSS로 써놨는데 코드 자체는 예측값 합의 제곱이라 잔차 제곱합이 아닌 것 같고…우선 pass
                        
                        ```r
                        gam_fun <- function(x){
                        rss <- sum(predict(gam1, newdata=data.frame(borrower_score=x, payment_inc_ratio=y))^2)
                        } 
                        ```
                        

### 5.6 마치며

- 분류
    - 어떤 레코드가 두 가지 이상의 범주 중 어디에 속하는지를 예측하는 프로세스
    - 한 클래스가 주요 관심 사항 - 이진 분류에서는 이 클래스가 1
- 관심 있는 클래스에 속할 확률인 경향 점수를 추정
- 관심 있는 클래스가 상대적으로 드물게 발생한다는 것이 문제
    - 이런 경우 모든 레코드에 대해 무조건 0이라고 예측하는 것이 높은 정확도를 얻게 됨
- 단순 정확도 외에 다양한 모델 평가 지표