# 4. 회귀와 예측

- [x]  선형회귀의 5가지 가정 체크 - 실무에서도 이루어지는가?
    
    [선형회귀의 5가지 가정](%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%8B%E1%85%B4%205%E1%84%80%E1%85%A1%E1%84%8C%E1%85%B5%20%E1%84%80%E1%85%A1%E1%84%8C%E1%85%A5%E1%86%BC%20ff033a49da424c99b35da829c33433f1.md)
    

---

- 용어
    - response variable(종속변수, 응답변수, 반응변수): 예측하고자 하는 변수
    - independent variable(독립변수, 예측변수, 피처, 속성): 응답치를 예측하기 위해 사용되는 변수
    - record(행, exampl,e instance): 하나의 특정 경우에 대한 입력, 출력을 담고 있는 벡터
    - 절편(intercept): 회귀지선의 절편. X=0일 때의 예측값. $b_0$
    - 회귀계수(regression coefficient): 회귀직선의 기울기. $b_1$, slope, 모수 추정값, 가중치
    - 적합값(fitted value): 회귀선으로부터 얻은 추정치 $\hat{Y}_1$
    - 잔차(residual): 관측값과 적합값의 차이, 오차
    - 최소제곱(least square): 잔차의 제곱합을 최소화하여 회귀를 피팅하는 방법. OLS(ordinary least squares)
    - RMSE(root mean squared error): 회귀 시 평균제곱오차의 제곱근. 회귀모형을 평가하는 데 가장 널리 사용되는 측정 지표
    - RSE(residual standard error): 평균제곱오차와 동일하지만 자유도에 따라 보정된 값
    - r-squared: 0에서 1까지 모델에 의해 설명된 분산의 비율
    - t-statistics: 계수의 표준오차로 나눈 예측변수의 계수. 모델에서 변수의 중요도를 비교하는 기준
    - weighted regression: 다른 가중치를 가진 레코드들을 회귀하는 방법
    - 예측구간: 개별 예측값 주위의 불확실한 구간
    - 외삽법(extrapolation): 훈련된 데이터 범위를 초과하여 새로운 값을 추정하려는 시도
    - dummy variable(가변수): 회귀나 다른 모델에서 요인 데이터를 사용하기 위해 0과 1의 이진변수로 부화(encoding, 변환)한 변수
    - reference coding(기준 부호화): 통계학자들이 많이 사용하는 부호화 형태. 여기서 한 요인을 기준으로 하고 다른 요인들이 이 기준에 따라 비교할 수 있도록 한다.
    - 원-핫 인코딩: 머신러닝 분야에서 많이 사용. 모든 요인 수진이 계속 유지된다. 다중선형회귀에는 부적합
    - 편차 부호화(deviation coding): 기존 수준과는 반대로 전체 평균에 대해 각 수준을 비교하는 부호화 방법
    - 변수 간 상관(correlated variables): 변수들이 같은 방향으로 움직이려는 경향. 예측변수끼리 서로 높은 상관성을 가질 때는 개별 계수를 해석하는 것이 어렵다
    - 다중공선성(multicollinearity): 예측변수들이 완벽하거나 거의 완벽에 가까운 상관성을 갖는다고 할 때, 회귀는 불안정하며 계산이 불가능
    - 교란변수(confounding variable): 중요한 예측변수이지만 회귀방정식에 누락되어 결과를 잘못되게 이끄는 변수
    - 주효과(main effect): 다른 변수들과 독립된 하나의 예측변수와 결과변수 사이의 관계
    - 상호작용(interaction): 둘 이상의 예측변수와 응답변수 사이의 상호 의존적인 관계
    - 표준화잔차(standardized residual): 잔차를 표준오차로 나눈 값
    - 특잇값(outlier): 나머지 데이터(혹은 예측값)와 멀리 떨어진 레코드(혹은 출력값)
    - 영향값(influential value): 있을 때와 없을 때 회귀방정식이 큰 차이를 보이는 값 혹은 레코드
    - 지렛대(레버리지): 회귀식에 한 레코드가 미치는 영향력의 정도
    - 비정규 잔차(non-normal residual):  정규분포를 따르지 않는 잔차는 회귀분석의 요건을 무효로 만들 수 있다. 데이터 과학에서는 별로 중요하게 다루지 않는다.
    - 이분산성(heteroskedasticity): 어떤 범위 내 출력값의 잔차가 매우 높은 분산을 보이는 경향(어떤 예측변수를 회귀식이 놓치고 있다는 것을 의미할 수 있음)
    - 편잔차그림(partial residual plot): 결과변수와 특정 예측변수 사이의 관계를 진단하는 그림
    - 다항회귀(polynomial regression): 회귀모형에 다항식(제곱, 세제곱 등) 항을 추가한 방식
    - 스플라인 회귀(spline regression): 다항 구간들을 부드러운 곡선 형태로 피팅
    - 매듭(know): 스플라인 구간을 구분하는 값들
    - 일반화 가법 모형(Generalized Additive Model, GAM): 자동으로 구간을 결정하는 스플라인 모델
    - 비선형회귀(nonlinear regression): 최소제곱 방법으로 피팅할 수 없는 모델
- 주요 개념
    - 회귀방정식은 응답변수 Y와 예측변수 X 간의 관계를 선형함수로 모델링한다.
    - 회귀모형은 적합값과 잔차, 즉 반응에 대한 예측과 그 예측 오차를 산출한다.
    - 회귀모형은 일반적으로 최소제곱법을 이용해 피팅한다.
    - 회귀는 예측과 설명 모두에 사용된다.
    - 다중선형회귀모형은 한 응답변수 Y와 여러 개의 예측변수(X1, …, Xp) 간의 관계를 나타낸다.
    - 모델을 평가하는 가장 중요한 지표는 RMSE(제곱근평균제곱오차)와 R^2이다
    - 계수들의 표준오차는 모델에 대한 변수 기여도의 신뢰도를 측정하는데 사용된다.
    - 단계적 회귀는 모델을 만드는데 필요한 변수들을 자동으로 결정하는 방법이다
    - 가중회귀는 방정식을 피팅할 때 레코드별로 가중치를 주기 위해 사용한다
    - 데이터 범위를 벗어나는 외삽은 오류를 유발할 수 있다.
    - 신뢰구간은 회귀계수 주변의 불확실성을 정량화한다
    - 예측구간은 개별 예측값의 불확실성을 정량화한다
    - R을 포함한 대부분의 소프트웨어는 수식을 사용하여 예측/신뢰구간을 기본 또는 지정된 출력으로 생성한다
    - 예측 및 신뢰 구간 생성을 위해 수식 대신 부트스트랩을 사용할 수도 있다
    - 요인변수는 회귀를 위해 수치형 변수로 변환해야 한다
    - 요인변수를 P개의 개별 값으로 인코딩하기 위한 가장 흔한 방법: P-1개의 가변수를 만들어 사용
    - 다수의 수준을 갖는 요인변수의 경우, 더 적은 수의 수준을 갖는 변수가 되도록 수준들을 통합해야
    - 순서를 갖는 요인변수의 경우, 수치형 변수로 변환하여 사용할 수 있다
    - 예측변수들 사이의 상관성 때문에, 다중선형회귀에서 계수들을 해석할 때는 주의해야 한다
    - 다중공선성은 회귀방정식을 피팅할 때, 수치 불안정성을 유발할 수 있다.
    - 교란변수란 모델에서 생략된 중요한 예측변수를 의미하며, 이에 따라 실제로 관계가 없는데 허위로 있는 것처럼 회귀 결과가 나올 수 있다.
    - 변수와 결과가 서로 의존적일 때, 두 변수 사이의 상호작용을 고려할 필요가 있다.
    - 특이점은 데이터 크기가 작을 때 문제를 일으킬 수 있지만, 주요 관심사는 데이터에서 문제점을 발견하든지 이상을 찾아내는 것이다.
    - 데이터 크기가 작을 때는 단일 레코드(회귀 특잇값 포함)가 회귀방정식에 큰 영향을 미치는 경우도 있다. 하지만 빅데이터에서는 이러한 효과가 대부분 사라진다.
    - 회귀모형을 일반적인 추론(p값 등)을 위해 사용할 경우 잔차 분포에 대한 특정 가정을 확인해야 한다. 하지만 보통 데이터 과학에서 잔차의 분포는 그렇게 중요하지 않다.
    - 편잔차그림을 사용하여 각 회귀 항의 적합성을 정량적으로 평가할 수 있다. 즉 대체 모델에 대한 아이디어를 얻을 수 있다.
    - 회귀분석에서 특잇값은 잔차가 큰 레코드를 말한다
    - 한 변수의 효과가 다른 변수의 수준에 영향을 받는다면 두 변수 사이의 상호작용을 고려할 항이 필요하다.
    - 다항회귀분석은 예측변수와 결과변수 간의 비선형 관계를 검증할 수 있다
    - 스플라인은 매듭들로 함께 묶여 있는 일련의 구간별 다항식을 말한다
    - 일반화가법모형(GAM)을 사용하여 스플라인의 매듭을 자동으로 결정할 수 있다.

---

## 4.1 단순선형회귀

- 한 변수와 또 다른 변수 사이의 관계에 대한 모델 제공
    - X가 증가 → Y도 증가 or X가 증가 → Y는 감소
- 상관관계와의 차이
    - 상관관계: 두 변수 사이의 관련 강도를 측정
    - 회귀: 관계 자체를 정량화

### 4.1.1 회귀식

- 단순선형회귀 → X가 얼만큼 변하면 Y가 어느 정도 변하는지 정확히 추정 가능
    - 상관관계에서는 X, Y가 바뀌어도 계수가 변하지 않음
- 선형관계를 이용해서 변수 X로부터 변수 Y를 예측
    
    $Y = b_0 + b_1X$
    
    - $b_1$: 계수 (coefficient)
- R 구현 및 결과 해석
    - Y`~`X: X를 통해 Y를 예측한다. 절편 자동 포함
    - Y`~`X -1 : 절편 제거
    - 구현
        
        ![Untitled](Untitled%2022.png)
        
        - Intercept (b_0)
            - 해석: 노동자가 면진?분진에 노출된 연수가 0일 때 예측되는 폐 기능 지표 값
        - Exposure (b_1)
            - 해석: 분진에 노출된 연수가 1씩 증가할 때마다 폐 기능 지표가 -4.185의 단위 감소
    - c.f. 잔차에 대하여
        - 회귀식 자체에는 Exposure만 변수로 들어간다. 그래서 같은 Exposure를 가진 행에 대해서는 적합값이 모두 동일
        - 그러나 실제 사람의 데이터에서는 Exposure 값이 달라도 폐 기능 지수가 서로 다르게 나타남 - 누구는 적합값보다 크고, 누구는 작음
        - `mutate(PEFR = ifelse(positive, PEFR_max, PEFR_min))`
            - (Exposure, positive) 조합으로 그룹화가 된 상태
            - 여기서 positive 값이 True인 행은 PEFR_max 즉, 그 조합의 그룹 내에서 가장 큰 PEFR을 가져오고
                - positive True인 행들은 우선 원래 PEFR 값이 적합값보다 큰 값
            - positive 값이 False 인행은 PEFR_min 즉, 그 조합의 그룹내에서 가장 값이 작은 PEFR을 가져옴
                - positive False인 행들은 원래 PEFR 값이 적합값보다 작은 값
            
            ➜ 같은 exposure 값이라고 해도 positive True에서 가장 큰 값은, 가질 수 있는 가장 큰 PEFR 수치를 의미하고, positive False 에서 가장 작은 값은, 가질 수 있는 가장 작은 PEFR 수치를 의미 
            
             ➜ 이 PEFR 칼럼을 가지고 행마다 수직선을 긋는다고 하면, 특정 exposure 값에서 실제 PEFR 수치가 어느 범위로 나타났는지 표현할 수 있음. 이 수직선들을 관통하는 회귀선이 그려지면, 잔차도 구할 수 있음 
            

### 4.1.2 적합값과 잔차

- 적합값(예측값), 잔차(예측 오차)
- 오차항을 포함한 회귀식
    
    $Y_i = b_0 + b_1X_i + e_i$
    
- 적합값 (^ 표기는 추정치를 의미)
    
    $\hat{Y_i} = \hat{b_0} + \hat{b_i}X_i$ 
    
- 잔차 = 원래 값 - 예측한 값
    
    $\hat{e_i} = Y_i - \hat{Y_i}$
    

### 4.1.3 최소 제곱

- 잔차들의 제곱한 값들인 잔차 제곱합을 최소화하는 선이 회귀선
    - ❤️‍🔥 잔차 제곱합 수식
        
        $$
        RSS = \sum^N_{i=1}(Y_i-\hat{Y}_i)^2  \\ = \sum^n_{i=1}(Y_i-\hat{b}_0-\hat{b}_1X_i)^2
        $$
        
    - 추정치 $\hat{b}_0$, $\hat{b}_1$은 RSS를 최소화하는 값
- 잔차제곱합을 최소화하는 방법: 최소제곱회귀, 보통최소제곱(OLS)
- 최소 제곱은 특잇값에 매우 민감

### 4.1.4 예측 대 설명(프로파일링)

- 과거-설명, 변수들 간의 전체적인 관계
    - 회귀방정식 기울기 $\hat{b}_1$ 추정에 초점
    - 예) 소비자 지출과 GDP 성장 간의 관계, 홍보 캠페인 효과
- 빅데이터 출현 이후-개별 결과 예측 모델 구성
    - 적합값 $\hat{Y_i}$에 초점
    - 예) 캠페인 크기에 따른 수익 변화 예측, SAT 점수에 따라 학부 평점 예측
- 회귀방정식 vs. 인과관계
    - 회귀모형: X의 변화가 Y의 변화 유도하도록 설정
        - 웹 광고에서 클릭 수와 전환률 간 명확한 관계
    - 그러나 광고 클릭과 판매 간의 명확한 관계가 나오더라도, 특히 판매 → 광고클릭 간 명확한 관계가 있더라도, 도메인 지식 고려하면 말이 안되는 관계라는 것을 알 수 있다.

## 4.2 다중선형회귀

- 예측변수가 여러 개인 상황
    
    $Y = b_0 + b_1X_1 + b_2X_2 + ... + b_pX_p + e$
    
    - Y 자체는 더 이상 직선 형태가 아니지만, 각 계수와 그 변수들 사이의 관계(b_p*X_p)는 여전히 선형이므로 선형 모형이라고 함
- 적합값
    
    $\hat{Y_i} = \hat{b_0} + \hat{b_1}X_{1, i} + \hat{b_2}X_{2, i} + ... + \hat{b_p}X_{p, i}$ 
    

### 4.2.1 킹 카운티 주택 정보 예제

- 다중선형회귀분석 대표적 사례
- 세금 산정 목적으로 주택 가치 추정
- 다중선형회귀 결과 읽기
    
    ![Untitled](Untitled%2023.png)
    
    - 다른 모든 변수 ($X_k$, k ≠ j)가 고정되었다고 가정했을 때, $X_j$가 변하는 정도에 따라 예측값 $\hat{Y}$도 계수 $b_j$에 비례해 변화
    - 예) 주택에 1제곱피트를 추가하면 예상가격이 229달러 정도 즐가할 것

### 4.2.2 모형 평가 ❤️‍🔥

1. **RMSE (root mean squared error)**
    
    $RMSE = \sqrt{{\sum^{n}_{i=1} (y_i-\hat{y_i})^2}\over n}$
    
    - DS 관점에서 가장 중요한 성능 지표
    - 예측된 $\hat{y}_i$ 값들의 평균제곱오차의 제곱근(루트)
        - datapoint 하나하나에 대한 오차를 제곱해서 다 더한 다음 평균, 루트 때린 결과
    - RSE (residual standard error)
        
        $RSE = \sqrt{\sum_{i=1}^{n}(y_i-\hat{y_i})^2\over{(n-p-1)}}$
        
        - p: 예측변수의 개수
        - 빅데이터 분야에서 RMSE와 RSE의 차이는 아주 작다
2. **$R^2$**
    
    $R^2 = 1 - {\sum^{n}_{i=1} (y_i-\hat{y_i})^2\over {\sum^{n}_{i=1} (y_i-\bar{y_i})^2}}$
    
    - 분자: 잔차 변동(Residual Variability). 회귀 모델이 설명하지 못한 종속 변수의 변동
    - 분모: 전체 변동(Total Variability). 실제 데이터 값들이 평균값으로부터 떨어진 정도의 제곱합
        - Y(y_i 전체 집합)의 분산에 비례함
    - 결정계수
    - 0~1까지 모델 데이터의 변동률을 측정한다
    - 모델이 데이터에 얼마나 적합한지 평가
    - adjusted $R^2$
        - 자유도를 고려 - 모델에 더 많은 예측변수(X, 칼럼)을 추가하는 것에 대해 효과적으로 페널티를 가한다
        - 공식
            
            ![Untitled](Untitled%2024.png)
            
        - 큰 데이터 집합에 대해서는 일반 R 제곱과 크게 다르지 않다고 함
3. **t-statistics**
    
    $t_b = {\hat{b} \over {SE(\hat{b})}}$
    
    - 계수별로 표준오차(standard error)와 t-통계량 값을 함께 출력
        
        ![Untitled](Untitled%2025.png)
        
    - 큰 t-통계량(절대값이 큰 값)은 회귀 계수 $\hat{b}$가 0과 유의미하게 다르다는 것을 시사
        - 해당 독립 변수가 종속 변수에 미치는 영향이 통계적으로 유의미하다는 것을 의미
    - t 통계량 & p값: 계수가 통계적으로 유의미한 정도를 측정
        - X와 Y를 랜덤하게 재배치했을 때 우연히 얻을 수 있는 범위를 어느 정도 벗어났는지
        - t값이 높을 수록(p값이 낮을 수록) 더 유의미한 X(예측 변수)

c.f. 함께 출력되는 p값 (Pr(>|t|)), F 통계량

- DS 쪽에서는 통계 해석이나 통계적 유의성 문제에 너무 깊이 관여 X
- DS에서는 모델에 예측 변수를 포함할 지 여부를 판단하기 위해 t 통계량을 유용하게 사용
- 높은 t값(낮은 p값): 예측 변수를 모델에 포함해야 된다는 것을 의미 ↔ t 값이 너무 낮으면 예측변수를 삭제할 수 있음을 의미

### 4.2.3 교차타당성검사

- 표본 내 통계적 회귀 측정 지표들
    - R**2, F통계량, p값
    - 모델을 구하는 데 사용했던 데이터를 똑같이 그대로 사용
- 표본 밖 유효성 검사
    - holdout data
    - 크기가 작은 홀드아웃 샘플의 변동성 → 불확실성
- Cross-validation
    - 여러 개의 연속된 홀드아웃 샘플을 둔다. k-fold cross-validation
        1. 1/k의 데이터를 홀드아웃 샘플로 따로 떼어놓는다.
        2. 남아 있는 데이터로 모델을 훈련시킨다
        3. 모델을 1/k 홀드아웃에 적용(점수를 매김)하고 모델 평가 지표를 기록한다
        4. 데이터의 첫번째 1/k을 복원하고, 다음 1/k을 따로 보관한다.
        5. 2~3단계를 반복한다 
        6. 모든 레코드가 홀드아웃 샘플로 사용될 때까지 반복한다.
        7. 모델 평가 지표들을 평균과 같은 방식으로 결합한다. 
    - fold: 훈련을 위한 샘플과 홀드아웃 샘플로 데이터를 나누는 것

### 4.2.4 모형 선택 및 단계적 회귀

- R에서는 회귀방정식에 범주형, 불 변수도 쉽게 추가할 수 있다
    - 파이썬에서는 숫자로 변환 해야 함
- 오컴의 면도날
    - 모든 것이 동일한 조건에서는 복잡한 모델 보다는 단순한 모델을 우선 사용해야 한다
- 실수형 칼럼, 범주형, 불 변수 칼럼을 모두 예측 변수로 사용하는 경우
    - R은 별도의 형 변환 없이 그대로 사용
    - Python은 `pd.get_dummies(dataframe, drop_first=True)` 사용
- 변수를 추가할 수록 학습 데이터에 대해 항상 RMSE는 감소하고 r**2는 증가한다
    - 좋은 쪽으로 지표가 나올 수 밖에 없어서 변수 추가 선택의 기준이 될 수 없다.

### 복잡도 고려하는 방법

- adjusted R**2
    
    $R^2_{adj} = 1 - (1-R^2) {n-1\over{n-P-1}}$
    
    - n: record 수 (행 수), P: 변수 개수 (칼럼 개수)
- AIC (Akaike’s information criteria)
    
    $AIC = 2P + nlog(RSS/n)$
    
    - [RSS: 잔차 제곱합, Residual Sum of Squares](4%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%8B%E1%85%AA%20%E1%84%8B%E1%85%A8%E1%84%8E%E1%85%B3%E1%86%A8%206ce52fcf859d47279e1ff9c8585d08f9.md)
    - 모델에 항을 추가할 수록 불이익을 주는 측정기준
    - 목표는 AIC를 최소화하는 모델을 찾는 것. 모델에 k(=P)개의 변수를 추가한다면, 2k만큼 불이익을 받게 된다.
    - AIC의 몇 가지 변형
        - AICc: 크기가 작은 표본을 위해 수정된 AIC
        - BIC: Bayesian Information Criteria. AIC와 비숫하지만 변수 추가에 더 강한 벌점을 주는 정보 기준
        - 맬로즈 C_p: 콜린 링우드 맬로즈가 제안한 변형
        - 일반적으로 표본 내측정 지표(training data)로 보고. DS에서는 모델 평가를 위해 홀드아웃 데이터를 사용하기 때문에 크게 걱정하지 않아도 된다.
- AIC를 최소로 하거나 adj R^2을 최대로 하는 모델 탐색
    - 부분집합회귀(all subset regression): 가능한 모델 모두 검색. computation cost 소모
    - stepwise regression: 단계적 회귀
        - backward elimination(후진 제거) : 전체 모델부터 시작하여 별로 의미 없는 변수들을 연속적으로 삭제
        - forward selection(전진 선택): 상수 모델에서 시작하여 연속적으로 변수 추가
            - R^2에 가장 큰 기여도를 갖는 예측변수를 하나씩 추가. 기여도가 통계적으로 더 유의미하지 않을 때 중지
        - 단계적 회귀분석: 예측변수를 연속적으로 추가/삭제하여 AIC를 낮추는 혹은 R 제곱을 높이는 모델 탐색
            - R에서 stepAIC 사용하는 경우
                - 모든 변수를 포함하는 기초 모델에서 시작
                - `step <- stepAIC(house_full, direction = 'both', trace=FALSE)` 결과
                    
                    ![Untitled](Untitled%2026.png)
                    
                    - 초기 모델은 11개 변수 사용했으나, stepAIC 이후에는 6개의 변수만 남았음
            - Python에서 stepAIC 사용하는 경우
                
                ![Untitled](Untitled%2027.png)
                
                - 원래 X.columns는 12개로 구성
                - step 이후에는 8개의 칼럼만 남음
                    - SqFtLot, NbrLivingUnits, YrRenovated, NewConstruction이라는 변수들이 삭제된 모델 선택
    - penalized regression(벌점 회귀)
        - 개별 모델 집합들을 명시적으로 검색하는 대신, 모델 적합 방정식에 많은 변수에 대해 모델에 불이익을 주는 제약 조건을 추가
        - 예측변수를 완전히 제거하는 대신, 계수 크기를 감소시키거나 경우에 따라 거의 0으로 만들어 벌점을 적용
        - 능형 회귀, 라소
    - 단계적 회귀분석, 부분집합회귀는 모델을 평가하고 조정하는 표본 내 방법
        - 선택된 모델이 과적합될 수 있고, 새 데이터를 적용할 때 잘 맞지 않을 수도
        - 교차타당성검사를 통해 모델의 유효성 알아보기
        - 선형 회귀분석에서는 데이터에 주어진 모델이 선형 전역 구조를 갖고 있기 때문에 일반적으로 과적합 문제가 크게 나타나지 않는다
            1. 모델의 단순성
                - 선형 회귀모델은 입력 변수와 출력 변수 간의 관계를 단순히 직선이나 평면으로 표현합니다. 이러한 단순성 덕분에 모델이 복잡한 패턴을 과도하게 학습하지 않기 때문에 과적합의 위험이 줄어듭니다.
                - 선형 회귀는 기본적으로 매개변수가 적습니다. 예를 들어,  p 개의 특성을 가진 데이터에 대해  p 개의 회귀계수와 하나의 절편(편향)만을 학습합니다.
            2. 선형 전역 구조
                - 선형 회귀모델은 전체 데이터에 대해 하나의 전역적인 직선(또는 평면)을 적합시키기 때문에, 데이터의 국소적 변동이나 노이즈를 과도하게 반영하지 않습니다.
                - 데이터의 각 특성에 대해 하나의 선형 가중치가 존재하므로, 모델이 특정 데이터 포인트나 작은 데이터 부분에 지나치게 민감하게 반응하지 않습니다.
            3. 규제의 용이성
                - 선형 회귀모델은 규제(regularization)를 적용하기 쉽습니다. 예를 들어, 릿지 회귀(Ridge Regression)나 라소 회귀(Lasso Regression)와 같은 기법을 사용하면 모델의 복잡도를 추가적으로 제어할 수 있습니다.
                - 규제를 통해 모델이 과도하게 큰 계수를 갖지 않도록 제한함으로써 과적합을 방지할 수 있습니다.
            4. 해석 가능성
                - 선형 회귀모델은 해석이 용이하다는 장점이 있습니다. 각 특성의 가중치를 통해 특성이 결과에 미치는 영향을 쉽게 이해할 수 있기 때문에, 모델이 실제로 데이터를 어떻게 예측하는지 쉽게 파악할 수 있습니다.
                - 이러한 해석 가능성 덕분에 모델의 학습 과정에서 잘못된 학습 패턴이 발견될 경우, 이를 쉽게 수정할 수 있습니다.

### 4.2.5 가중 회귀

- 가중회귀의 유용성
    - 서로 다른 관측치를 다른 정밀도로 측정했을 때, 역분산 가중치를 얻을 수 있다
    - 가중치 변수가 집계된 데이터의 각 행이 나타내는 원본 관측치의 수를 인코딩 하도록, 행이 여러 경우를 의미하는 데이터를 분석할 수 있다.
        - 어떤 행의 한 칼럼 값이 50명의 평균이고, 또 다른 행의 한 칼럼 값은 40명의 평균일 때, 인원수 칼럼 값을 가중치로 사용해서 각 행이 실제 데이터에서 차지하는 비중을 반영해야 함
- 예-주택 가격에서 오래된 매매 정보일 수록 최근 정보보다는 신뢰하기 어려움
    - DocumentDate 칼럼 값을 이용해서 2005년 이래 지난 연수를 가중치로 사용 가능
    

## 4.3 회귀를 이용한 예측

- DS에서 회귀의 주된 목적은 예측. 통계학에서는 설명을 위한 모델링

### 4.3.1 외삽의 위험

- 시계열 예측을 위해 회귀를 고려하지 않는다
    - 회귀모형을 데이터 범위를 초과하면서까지 외삽하는 데 사용해서는 안된다
    - 예) 집값 데이터를 가지고 얻은 model_lm을 가지고 어떤 크기의 공터 가격을 예측할 순 없다
        - 모델 피팅 시에 건물이 있는 구획의 데이터만 사용했기 때문에
        - 빈 땅에 해당하는 레코드는 없음

### 4.3.2 신뢰구간과 예측구간

- 통계학 - 변동성(불확실성)을 이해하고 측정하는 것 포함
    - t 통계량, p 값 → 변수 선택을 위해 활용됨
- 더 유용한 지표 - 신뢰구간
    - 회귀계수와 예측을 둘러싼 불확실성 구간
    - 부트스트랩을 이용해서 쉽게 이해 가능
- 회귀 파라미터(계수)의 신뢰구간을 생성하기 위한 부트스트랩 알고리즘
    - P개의 예측변수(열)와 n개의 레코드(행)이 있는 데이터
    1. 각 행(결과 변수 포함)을 하나의 티켓으로 생각하고 개수가 모두 n개인 티켓을 박스에 넣었다고 가정
    2. 무작위로 티켓을 뽑아 값을 기록하고 다시 박스에 넣는다
    3. 2번 과정을 n번 반복한다. 이를 통해 부트스트랩 resample을 하나 만든다
    4. 이 부트스트랩 표본을 가지고 회귀모형을 구한다. 추정된 계수들을 기록한다. 
    5. 2~4번 과정을 1000번 반복한다
    6. 이제 각 계수에 대해 1000개의 부트스트랩 값을 갖게 된다. 각각에 대해 적합한 백분위수를 구한다 (90% 신뢰구간을 위해 5번째에서 95번째 백분위수를 구한다)
    - R의 Boot 함수를 사용
- DS에서는 예측된 y값($\hat{Y_i}$)의 구간이 제일 큰 관심사
    - 예측값의 불확실성은 두 가지 원인에서 비롯
    1. 무엇이 적합한 예측변수인지, 계수가 얼마인지에 따른 불확실성 
    2. 개별 데이터값에 존재하는 추가적인 오류 
    - 개별 데이터 값의 오차
        - 회귀방정식이 무엇인지 정확히 알았다하더라도, 주어진 예측변수 값에 대한 실제 결과값은 달라질 수 있다
            - 예) 같은 예측변수 값이라고 해도 - 방 8개, 실 평수 6500제곱피트, 욕실 3개, 지하실 있음- 집마다 가격이 다를 수 있음
        - 이런 개별 오차를 적합값(fitted value, 예측값)으로부터의 잔차로 모델링할 수 있다
- 회귀모형에 따른 오차와 개별 데이터 값에 따른 오차를 모두 모델링하기 위한 부트스트랩 알고리즘
    1. 각 행(결과 변수 포함)을 하나의 티켓으로 생각하고 개수가 모두 n개인 티켓을 박스에 넣었다고 가정
    2. 무작위로 티켓을 뽑아 값을 기록하고 다시 박스에 넣는다
    3. 2번 과정을 n번 반복한다. 이를 통해 부트스트랩 resample을 하나 만든다
    4. 이 부트스트랩 표본을 가지고 회귀모형을 구한다. 새로운 값을 예측한다
    5. 원래의 회귀 적합도에서 임의로 하나의 잔차를 취하여 예측값에 더하고 그 결과를 기록한다 
    6. 1~5 단계를 1000번 반복한다
    7. 결과의 2.5번째 백분위수와 97.5번째 백분위 수를 찾는다 
- 예측구간 vs. 신뢰구간
    - 예측구간: 하나의 값(개별 관측치)에 대한 불확실성과 관련
        - 예-“독립 변수 X가 특정 값일 때, 종속 변수 Y의 개별 관측치가 95% 신뢰수준에서 이 범위 내에 있을 것이다.”
        - 일반적으로 같은 값에 대해 신뢰구간보다 훨씬 넓다
            - 개별 관측치가 평균 응답 값보다 더 큰 변동성을 가지기 때문
        - 일반적으로 DS에서는 특정 개별 예측에 관심이 있으므로 예측구간이 더 적절
    - 신뢰구간: 여러 값에서 계산된 평균(계수나 평균 응답 값)이나 다른 통계량과 관련

## 4.4 회귀에서의 요인 변수

- factor variable: 범주형 변수
    - 개수가 제한된 이산값(Discrete Value, 특정한 값들로 이루어진 데이터 ↔ 연속적인 범위)
    - indicator variable: 이진 변수. 요인 변수의 특수한 경우
- 회귀분석에는 수치 입력이 필요하기 때문에, 모델에 사용할 수 있도록 요인변수를 다시 수치화해야
    - 변수를 이진 가변수(dummy variable)의 집합으로 변환
- **c.f. 통계학에서의 부호화 기법**
    1. 더미 변수 (Dummy Variables) 인코딩:
        - 범주형 변수를 이진 변수(0 또는 1)로 변환하는 기법입니다. 예를 들어, 성별 변수(“남성”, “여성”)를 “남성” = 1, “여성” = 0으로 변환합니다.
        - 회귀 분석, 로지스틱 회귀 등에서 범주형 변수를 포함할 수 있도록 도와줍니다.
    2. 원-핫 인코딩 (One-Hot Encoding):
        - 범주형 변수를 여러 개의 이진 변수로 변환하는 기법입니다. 예를 들어, 색상 변수(“빨강”, “초록”, “파랑”)를 “빨강” = [1, 0, 0], “초록” = [0, 1, 0], “파랑” = [0, 0, 1]로 변환합니다.
        - 주로 머신러닝 알고리즘에서 범주형 데이터를 처리할 때 사용됩니다.
    3. 라벨 인코딩 (Label Encoding):
        - 범주형 변수를 특정 순서대로 정수로 변환하는 기법입니다. 예를 들어, “고양이”, “개”, “새”라는 범주형 변수를 “고양이” = 0, “개” = 1, “새” = 2로 변환합니다.
        - 순서가 있는 범주형 변수의 경우 유용하지만, 순서가 없는 경우에는 더미 변수나 원-핫 인코딩이 더 적합합니다.
    4. 비닝 (Binning) 또는 디스크리타이제이션 (Discretization):
        - 연속형 변수를 일정한 구간으로 나누어 범주형 변수로 변환하는 기법입니다. 예를 들어, 나이를 “0-18”, “19-35”, “36-50”, “51+“와 같은 구간으로 나누는 경우입니다.
        - 연속형 변수의 분포를 범주형 변수로 간단히 표현할 때 사용됩니다.
    5. 표준화 (Standardization)와 정규화 (Normalization):
        - 표준화는 데이터를 평균 0, 분산 1로 변환하는 과정이며, 정규화는 데이터를 0과 1 사이의 값으로 변환하는 과정입니다.
        - 통계 분석 및 머신러닝 모델에서 데이터의 스케일을 조정하여 모델 성능을 향상시킬 수 있습니다.

### 4.4.1 가변수 표현

- 다중공선성 문제 (multicollinearity)
    - P의 개별 수준을 갖는 요인 변수(category 개수가 P개인 경우), 보통 P-1개의 열을 갖는 행렬로 표시
    - 회귀 모형에 일반적으로 절편이 포함되기 때문
        - 절편의 이해
            - 다른 X가 모두 0일 때 Y의 예상값
            - X의 효과를 더하기 전 기본적인 수준을 설정하는 기준선 역할
            - 회귀 모형이 예측을 시작하는 출발점
    - 절편이 있기 때문에 P-1개의 이진변수의 값을 정의하고 나면, P번째 값을 알 수 있음
        - 예
            
            $y = \beta_0 + \beta_1 \cdot \text{PropertyTypeApartment} + \beta_2 \cdot \text{PropertyTypeCondo}$
            
            기준변수가 House이고, beta1, beta2의 값이 정해지고 나면, house 값을 알 수 있음
            
            beta1, beta2가 모두 0이면 beta_zero가 1 (왜냐면 셋 중 하나니까)
            
            beta1이나 beta2가 1이면 beta_zero는 0 
            
        
        → P번째 열을 추가하면 오류 발생 
        
- R의 기본 표현법은 첫번째 요인 수준을 기준(reference)로 하고, 나머지 수준을 이 기준에 상대적인 것으로 해석한다
    - 예시 결과
        
        ![Untitled](Untitled%2028.png)
        
    - 해석
        - PropertyType Single Family와 Townhouse에 대응하는 두 계수는 볼 수 있다.
        - 그러나 Multiplex에 대한 정보는 없다
            - 왜냐면 Single Family 계수가 0이고 Townhouse 계수가 0일 때 암묵적으로 정의되기 때문에
        - Single Family, Townhouse의 계수는 Multiplex에 상대적인 값
            - Single Family인 주택은 Multiplex인 주택보다보다 $85000정도 가치가 더 낮고, Townhouse는 $150000 정도 가치가 더 낮다
        - 절편
            - 모든 독립 변수(SqFtTotLiving, SqFtLot, Bathrooms, Bedrooms, BldgGrade)가 0이고, PropertyType이 “Multiplex”일 때 예상되는 AdjSalePrice
                - 범주형 변수의 경우, 기준 범주(reference category)의 값으로 설정되고, 연속형 변수의 경우, 값이 0으로 설정된 경우
            - 회귀 모델을 피팅하는 과정에서 절편 \beta_0도 다른 회귀 계수와 함께 최적화되기 때문에 하나의 파라미터로 간주한다
        - ❗️헷갈렸던 점
            - 상대적이라는 의미는 하나의 범주형 변수에 속하는 요인 수준에 한정된다
                - 다른 연속형 변수의 계수는 기준 범주에 대한 상대적 효과가 아님
                - 연속형 변수의 계수는 그 변수 자체의 독립적인 영향을 나타냅니다.
            - 절편이 Multiplex 기준 값이긴 하지만, 그렇다고 SqftTotLiving 계수가 Multiplex와 관련있는 값은 아니다. Y인 가격과 X인 SqftTotLiving 사이의 관계
            - Multiplex 대비 상대적이라는 건 같은 요인 수준인 Single Family, Townhouse에 한정
- c.f. 그 밖의 요인 변수 코딩 법들
    - constrast coding(대비 부호화)
        - 편차 부호화 방법(총합 대비) : 각 수준을 전반적인 평균과 비교
        - 다항식 부호화: 순서가 있는 요인 변수에 적합

### 4.4.2 다수의 수준을 갖는 요인 변수들

- 이진변수는 문제가 안되지만 우편번호를 생각해보자
    - 0~43000인데 범위에서 데이터에 존재하는 번호는 총 79개
    - 모든 수준을 포함하면 칼럼 수가 엄청 많아짐 → 모델 자유도가 엄청 올라감
    - 모든 요소를 유지 or 수준 통합 결정해야
- 우편번호는 주택 가격에 대한 위치 효과를 볼 수 있는 변수
    - 수준 통합한다고 하면 - 대도시 지역의 경우 처음 두 자리, 세자리만 사용 가능
    - 킹카운티의 경우 거의 모든 판매가 980xx, 981xx라서 큰 도움 X
- 대안
    - 매매 가격과 같은 다른 변수에 따라 우편번호를 그룹으로 묶기
    - 초기 모델의 잔차를 사용해서 우편 번호 그룹을 만드는 방법
    - c.f. 잔차 중간값을 사용해서 요인 수준을 그룹화 하는 것의 효용
        - 특정 요인 수준이 모델 예측에서 체계적인 편향을 가지는지 여부를 평가할 수 있습니다.
            - 어떤 요인 수준이 다른 수준과 비교하여 더 나은 예측력을 갖는지 또는 문제가 있는지를 쉽게 파악
        - 예를 들어, 어떤 요인 수준에서 잔차 중간값이 다른 요인 수준보다 크게 나타난다면, 해당 요인 수준에서 모델이 체계적으로 과소 또는 과대 평가하고 있을 수 있습니다.
- 회귀 적합화를 돕는 데 잔차를 사용한다는 개념은 모델링 과정의 기본 단계

### 4.4.3 순서가 있는 요인 변수

- ordered factor variable, ordered categorical variable
- 일반적으로 숫자 값으로 변환하여 그대로 사용 → 그냥 요인변수로 다루면 잃어버릴 수 있는 순서 정보 유지

## 4.5 회귀방정식 해석

- DS에서 회귀의 가장 중요한 용도: 일부 종속변수(결과변수)를 예측하는 것
- 방정식 자체로부터 통찰 → 예측변수와 결과 간 관계의 본질을 이해

### 4.5.1 예측변수 간 상관

- step_lm 결과
    
    ![Untitled](Untitled%2029.png)
    
- 해석
    - Bedrooms
        - 계수가 음수. 침실 개수를 늘릴 수록 가치가 감소?
        
        → 예측 변수들이 서로 연관되어 있기 때문 
        
        - 집이 클 수록 침실이 더 많은 경향, 침실 수보다는 주택의 크기가 주택 가격에 더 큰 영향
        - 똑같은 크기의 두 집이 있다고 하면, 작은 크기의 침실이 여러 개 있는 것을 선호하지 않는 것이 합리적
- 상호 연관된 예측 변수
    - 회귀계수의 부호와 값의 의미를 해석하기 어려움. 추정치의 표준오차가 커짐
    - step_lm에서 상관관계가 있는 변수들 제거하고 난 후의 결과
        - 침실 수, 평수, 욕실 수 제거
        
        ![Untitled](Untitled%2030.png)
        
        - Bedroom에 대한 계수가 이제는 양수로 나옴
            - 이제 침실 수가 실제 주택 크기에 대한 대리 변수로 작용
- 교란변수
    - 주택의 위치를 고려할 변수가 따로 없는 상태에서, 서로 다른 유형의 지역 정보가 섞여 있음
    - 위치 정보가 교란변수

### 4.5.2 다중공선성

- 변수 상관의 극단적인 경우 나타나는 다중공선성 - 예측변수 사이의 중복성 판단 기준
- 완전 다중공선성
    - 한 예측변수가 다른 변수들의 선형 결합으로 표현됨
- 다중공선성 발생 시나리오
    - 오류로 인해 한 변수가 여러번 모델에 포함
    - 요인변수로부터 P-1개가 아닌 P개의 가변수가 만들어진 경우
    - 두 변수가 서로 거의 완ㅂ겨하게 상관성이 있는 경우
- 회귀분석에서는 다중공선성이 사라질 때까지 변수를 제거해야 함
    
    c.f. 트리, 클러스터링, kNN 같은 선형회귀 유형이 아닌 방법에서는 문제 X. 여기서는 P개의 가변수 유지하는 게 좋음. 그치만 예측변수의 비중복성 유지하는 것이 여전히 미덕 
    

### 4.5.3 교란변수

- 변수 상관: Y(응답변수)와 비슷한 예측 관계를 갖는 다른 변수가 (X, 예측변수) 포함되는 바람에 비롯된 문제
- 교란변수: 회귀방정식에 중요한 변수가 포함되지 못해서 생기는 누락의 문제
- 교란변수 ZipGroup(주택 위치를 대리)을 추가한 뒤의 결과 해석
    - 가장 비싼 우편번호 그룹의 주택 가격이 약 34만달러나 더 높음
        
        ![Untitled](Untitled%2031.png)
        
        - default가 가장 싼 지역의 우편번호 그룹 0
        - group 1~4로 갈 수록 계수가 커지는 것이 보임.
            - 계수에 곱해지는 숫자는 0 or 1. indicator.
        - 가장 비싼 지역의 그룹 ZipGroup_4의 경우 group 0에 비해 약 34만 달러 더 높음
    - SqFtLot, Bathrooms의 계수는 이제 양수. 욕실 하나 추가하면 5928 달러가 증가함
    - Bedrooms에 대한 계수는 여전히 음수

### 4.5.4 상호작용과 주효과

- 모델에서 주효과만 사용한다면, 예측변수와 응답변수 간의 관계가 다른 예측변수들에 대해 독립적이라는 암묵적인 가정 → 사실이 아니다
    - 예: 킹카운티 주택 데이터에 적합한 모델은 ZipCode를 비롯한 여러 변수를 주효과로 포함
        - 주택 크기와 매매 가격 간의 관계가 위치에 달려 있다고 가정하는 것은 자연스러운 일
            - 임대료가 싼 지역에 지어진 큰 집의 가치 ≠ 비싼 지역에 지어진 큰 집의 가치
- R의 * 연산자: 모델의 변수 상호작용 포함 가능
    - 예-주택 크기와 우편번호 그룹 간의 상호 작용 고려
        - 결과
            
            ![Untitled](Untitled%2032.png)
            
        - 새로 추가된 정보들: `SqFtTotLiving:ZipGroup2`
        - 해석
            - 가격대가 가장 낮은 ZipGroup에서 집에 대한 기울기는 제곱피트당 115달러
                - 주효과 SqFtTotLiving에 해당하는 계수와 동일
                    - SqFTTotLiving:ZipGroup1 = SqFtTotLiving
            - 절편 자체의 의미는 상호작용 항 유무에 영향을 받지 않는다
                - 범주형은 기본 범주(ZipGroup1, PropertyType = Multiplex)
                - 연속형 변수는 값이 모두 0
            - 가장 비싼 ZipGroup에 대한 계수
                - 주효과 계수(115) + SqFtTotLiving:ZipGroup5 계수(227) = 342
            - (🤔 헷갈림) 상호작용항이 추가되면 주효과에 대한 계수도 달라진다
                - 예) ZipGroup은 상호작용항 없을 때 ZipGroup 커질 수록 숫자 커질 수록 증가했지만, 상호작용항이 생기고 나니까 ZipGroup5에서 오히려 음수가 나왔다
                
                ![Untitled](Untitled%2033.png)
                
                - 상호작용 항이 있는 경우 $\beta_1$이 여전히 SqFtTotLiving의 주 효과를 나타내지만, 이는 ZipGroup이 기준 그룹일 때만 해당
                - $\beta_2$는 여전히 ZipGroup의 주 효과를 나타내지만, 이는 SqFtTotLiving이 0일 때만 해당
                - 상호작용 항 $\beta_3$는 SqFtTotLiving과 ZipGroup이 결합할 때의 추가적인 효과를 설명합니다.
- 상호작용 항들을 이용한 모델 선택
    - 다수의 변수가 존재하는 경우, 어떤 상호작용을 고려해야 할지 결정하기 어려움
        - 사전 지식, 직관
        - 단계적 선택(step AIC)
        - 벌점을 부여하는 방식의 회귀 방법 → 자동으로 가능한 상호작용 필터링
        - 랜덤 포레스트, 그래디언트 부스팅 트리 같은 트리 모델
            - 자동으로 최적의 상호작용 항들을 걸러냄

## 4.6 회귀진단

- 설명을 위한 모델링(연구 목적)
    - 측정 지표 고려하여 매 단 계마다 모델이 데이터에 얼마나 적합한지 평가
    - 잔차 분석이 기본

### 4.6.1 특잇값

- 실제 y값이 예측된 값에서 멀리 떨어져 있는 경우
    - 표준화잔차(잔차를 표준오차로 나눈 값)을 조사해서 발견
- 표준화잔차
    - 회귀선으로부터 떨어진 정도를 표준오차 개수로 표현한 값
    - 추가설명
        
        ![Untitled](Untitled%2034.png)
        
- 특잇값 레코드 확인, 잔차 확인
- 빅데이터의 경우, 새로운 데이터를 예측하기 위한 회귀분석에서 특잇값이 그렇게 문제가 되지 않는다

### 4.6.2 영향값

- influential observation(주영향관측값)
    - 회귀모형에서 제외됐을 때 모델에 중요한 변화를 가져오는 값
    - 잔차가 크다고 해서 모두 주영향 관측값이 되는 것은 아님
    - 예) p.205 그림 4-5 : 잔차가 그렇게 크지 않아도 주영향 관측값이 되는 경우
        - 실선: 모든 데이터를 고려한 회귀선 vs. 점선: 오른쪽 위의 한 점을 제거했을 때 회귀선
        - 오른쪽 위의 한 점
            - 회귀 결과에 큰 영향을 미치지만, 원래 회귀에서 큰 특잇값으로 나타난 것은 아님
                - 왜냐면 잔차가 엄청 큰 경우는 아니기 때문에
            - 회귀에 대한 높은 레버리지를 가진 값
- 표준화잔차 외에 회귀분석에서 단일 레코드의 영향력을 결정하는 몇 가지 지표
    - hat-value
        - 2*(P+1)/n 이상의 값들은 레버리지가 높은 데이터값
    - Cook’s distance
        - 레버리지와 잔차의 크기를 합쳐서 영향력 판단
        - 4/(n-P-1)보다 크면 영향력이 높다고 보는 편
    - influence plot, bubble plot
        - 표준화잔차, 햇 값, 쿡의 거리를 모두 한 그림에 표현
    - c.f. R에서 영향력 큰 레코드 제외하고 다시 피팅하려면 lm(…, `subset = 조건추가`, …)
        - 조건은 쿡의 거리를 각 레코드 별로 구해둔 다음, cooks_D > 0.08 이런 식으로 Bool vector가 나오도록 설정
- 회귀모형을 구하는 목적에 따라 레코드별 영향력 확인이 유용할 수도 아닐 수도
    - 새로 들어오는 값에 대해 믿을 만한 예측값을 얻기 위함이라면
        - 데이터의 크기가 작을 경우에만 영향력이 큰 관측 데이터를 확인하는 작업이 유용
    - 빅데이터에서는 어떤 한 값이 회귀방정식에 엄청난 변화를 가져오기란 쉽지 않다
        - 회귀 결과 여전히 특잇값들이 존재한다 하더라도
    - 이상 검출이 목적이라면 필수

### 4.6.3 이분산성, 비정규성, 오차 간 상관

- DS에서는 대부분의 문제에서 잔차 분포에 너무 많은 신경을 쓸 필요는 없다
    - OLS는 다양한 분포 가정하에서 편향성도 없고, 경우에 따라 ‘최적’이라고 할 수있는 추정을 제공
    - 전통적인 통계에서는 중요
        - 잔차의 분포는 공식적인 통계적 추론(가설검정 및 p값)의 유효성과 관련
        - 오차가 정규분포를 다른다는 것 = 모델이 완전하다는 신호
            - 정규분포를 따르지 않으면 모델에서 뭔가가 누락되었을 수 있음을 의미
    - DS에서는 예측 정확도가 중요
    - c.f. 전통적인 통계 vs. 데이터 사이언스
        - 전통적인 통계학자들과 데이터 과학자들의 관점 차이는 다음과 같습니다:
            - **목적**: 통계학은 주로 데이터에서 통계적 패턴을 발견하고, 이를 통해 가설을 검정하거나 인과 관계를 밝히는 데 중점을 둡니다. 반면, 데이터 과학은 실용적인 예측 모델을 개발하는 데 중점을 둡니다.
            - **모델링 접근**: 통계학에서는 모델이 데이터에 잘 맞는지, 잔차가 정상 분포를 따르는지 등의 가정을 중요하게 여깁니다. 데이터 과학에서는 이러한 가정보다 모델이 실제 데이터에 대해 얼마나 잘 예측하는지에 더 집중합니다.
            - **평가 기준**: 통계학에서는 p값, 신뢰 구간 등의 통계적 지표를 중시하지만, 데이터 과학에서는 정확도, 정밀도, 재현율, F1 스코어 등의 예측 성능 지표를 더 중시합니다.
        - 잔차에 대한 가정이 DS에서 덜 중요하게 여겨지는 이유
            - **복잡한 모델**: 데이터 과학에서는 딥러닝, 랜덤 포레스트 등의 복잡한 모델을 사용합니다. 이러한 모델들은 잔차가 독립적이고 동일한 분포를 따른다는 가정을 필요로 하지 않습니다.
            - **모델 성능**: 예측 모델의 성능이 실제 데이터를 얼마나 잘 맞추는지가 중요하며, 잔차의 분포는 이러한 성능에 큰 영향을 미치지 않습니다.
            - **실제 문제 해결**: 실제 문제에서는 잔차의 분포보다는 모델이 얼마나 잘 작동하는지가 더 중요합니다. 예를 들어, 추천 시스템에서 추천의 정확도가 중요하지, 잔차가 정규분포를 따르는지는 중요하지 않습니다.
- 형식적 추론이 완전히 유효하려면 잔차에 대한 가정이 필요
    1. 동일한 분산을 가짐 (등분산 가정)
    2. 정규분포를 따름 (정규성 가정)
    3. 서로 독립 (독립성 가정)
- DS에서 신경 쓰는 것 한가지는 잔차에 대한 가정을 기반으로 예상 값에 대한 신뢰구간을 계산하는 방법
    - 예측구간 구할 때 모수적 접근 방법으로 구하려면 - 정규분포 따라야 95% 신뢰구간 구할 때 +- 1.96 * 표준오차 이런 식으로 구할 수 있으니까 - 필요하다는 것 같음
    - chatGPT 부가설명
        1. **잔차에 대한 가정**: 잔차는 실제 값과 모델이 예측한 값의 차이입니다. 잔차의 분포에 대한 가정(예: 잔차가 정규분포를 따른다)은 예측 값의 신뢰구간을 계산하는 데 중요합니다.
        2. **예측 값의 신뢰구간**: 예측 값의 신뢰구간은 모델이 예측한 값이 실제 값 주변에서 어느 정도 범위 내에 있을지를 나타내는 범위입니다. 이 신뢰구간은 예측의 불확실성을 정량화하는 데 사용됩니다.
        3. **방법**: 신뢰구간을 계산하기 위해 잔차가 어떤 분포를 따르는지 가정합니다. 가장 일반적인 가정은 잔차가 정규분포를 따른다는 것입니다. 이 가정을 바탕으로 모델의 예측 값 주변에서 신뢰구간을 계산할 수 있습니다.
- 이분산성
    - 다양한 범위의 예측값에 따라 잔차의 분산이 일정하지 않은 것을 의미
    - 어떤 일부분에서의 오차가 다른 데 보다 훨씬 크게 나타남
    - 데이터 시각화를 통해 잔차들의 분포 분석 가능
- 이분산성 데이터 시각화 결과 및 해석
    - 시각화 결과1: 절대잔차와 예측값의 관계
        
        ![Untitled](Untitled%2035.png)
        
    - 잔차의 분산은 고가의 주택일수록 증가하는 경향
    - 가격이 낮은 (일부 구간)주택의 경우에도 마찬가지로 큰 편
    
    → 회귀모형 lm_98105는 이분산성 오차를 갖고 있다고 볼 수 있다  
    
    - 추세선만 봐도 수평선이 아니고, 그 구간의 넓이도 가격 따라 차이가 큼
    - 시각화 결과2: 표준화잔차에 대한 히스토그램
        
        ![Untitled](Untitled%2036.png)
        
        - 정규분포보다 확실히 더 긴 꼬리. 더 큰 잔차에 대해 약간의 왜곡(큰 잔차 쪽으로 약간 더 기울어져있다-X 축 그래프만 봐도 6까지 오른쪽으로 펼쳐져 있음)
- DS에서 과학자는 이분산성에 관심을 가져야 할까?
    - 이분산성의 의미
        - 예측값이 어떤 경우에는 맞고, 어떤 경우에는 틀리다는 것 = 얻은 모델이 불완전하다
        - 예) lm_98105의 이분산성 → 가격이 너무 높거나 낮은 주택에 대해서는 회귀모형이 잘 설명하지 못한다는 것을 나타냄
- 통계학자들은 오차가 독립적이라는 가정을 점검하는 경우
    - c.f. 오차와 잔차의 혼용
        - “오차(errors)“는 실제 값과 모델의 참 값 간의 차이로, 보통 우리가 알 수 없는 값입니다
        - “잔차(residuals)“는 실제 값과 모델이 예측한 값 간의 차이로, 데이터에서 관찰하고 계산할 수 있는 값입니다.
    - 더비-왓슨 통계량 사용
        - 시계열 데이터를 다루는 회귀분석에서 유의미한 자기 상관이 있는지 탐지
        - 회귀모형 오차들 간에 상관관계가 있는 경우, 단기 예측에 유용하기 때문에 모델 만들 때 함께 고려해야
        - 더 장기적인 예측이나 설명 모델이 필요할 경우, 너무 자세하게 과도한 자기상관 데이터는 오히려 방해 → 데이터를 평활화하거나 데이터 수집을 듬성듬성하는 것이 필요
        - chatGPT 보충 설명
            - 만약 우리가 하루의 주식 가격을 예측하는 모델을 만들었다고 가정해 봅시다.
                - 예측값과 실제 값의 차이를 잔차라고 합니다.
                - 오늘의 잔차가 내일의 잔차와 관련이 있다면, 즉 오늘 잔차가 크면 내일 잔차도 클 가능성이 높다면 이는 자기 상관이 있는 것입니다.
                - 더빈-왓슨 통계량은 이러한 자기 상관이 있는지를 검사하여 우리가 만든 모델이 적절한지 확인하는 데 도움을 줍니다.
            - 더빈-왓슨 통계량 range : 0~4
                - 2에 가까우면 자기 상관이 없음
                - 0에 가까우면 양의 자기 상관(오차들이 서로 비슷한 방향으로 변화함)을 의미
                - 4에 가까우면 음의 자기 상관(오차들이 서로 반대 방향으로 변화함)을 의미
- 회귀가 분포 가정 중 한 가지만 위반해도 신경써야 할까?
    - DS에서 가장 중요한 것은 보통 예측 정확도 > 이분산성에 대한 검토
    - 이분산성 검토 결과 모델이 설명하지 못하는 데이터가 있을 수 있다
    - 그러나 단순히 공식 통계적 추론(p값, F 통계량 등)을 입증하기 위해서 분포 가정을 만족시키는 것은 DS에서 그리 중요한 일이 아님
- 산점도 평활기(scatterplot smoother)
    - 회귀분석: 응답변수와 예측변수 간의 관계를 모델링
    - 회귀모형 평가할 때 두 변수 사이의 관계를 시각적으로 강조하기 위해 산점도 평활기 사용
    - loess 함수
        - 일련의 구간별 지역 회귀모형을 구한 후 그것들을 연속적으로 부드럽게 만들어낸다 = smoothing, 평활화
        - 예-절대잔차와 예측값 간의 관계를 부드럽게 나타낸 곡선

### 4.6.4 편잔차그림과 비선형성

- 편잔차그림
    - 예측모델이 X, Y 간의 관계를 얼마나 잘 설명하는지 시각화
    - 하나의 예측변수와 응답변수 사이의 관계를 다른 모든 예측변수로부터 분리
    - 편잔차 - 단일 예측변수를 기반으로 한 예측값과 전체를 고려한 회귀식의 실제 잔차를 결합하여 ‘만든 결과’
    - 예측변수 $X_i$의 편잔차
        - 일반잔차 + $X_i$와 연관된 회귀항을 더한 값 = 잔차 + $\hat{b_i}X_i$
        - $\hat{b}_i$: 회귀계수의 추정치
        - R의 predict 함수를 이용해서 개별 회귀 항 $\hat{b_i}X_i$을 얻을 수 있음
    - R - predict - type = ‘terms’
        - 전체 예측값:  예측값은 각 설명 변수의 계수와 그 설명 변수의 값의 곱을 더한 값으로 계산
            - $\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2$
        - 각 설명 변수의 기여도 : 특정 변수의 값 * 회귀 식에서 나온 특정 변수의 계수 =  $\hat{b_i}X_i$
            - $\beta_1x_1$ 또는 $\beta_2x_2$
    - 그림
        
        ![Untitled](Untitled%2037.png)
        
    - 해석
        - 편잔차는 SqFtTotLiving 변수가 주택 가격에 얼마나 영향을 미치는지 보여줌
        - 해당 변수와 실제 가격 사이의 관계는 점선 → 비선형
        - 회귀선(실선)에 따르면
            - 1000제곱피트보다 작은 평수에 대해서는 가격을 실제보다 낮게 추정하고,
            - 2~3000 제곱피트에 집에 대해서는 더 높게 추정 (실선이 점선보다 우위)
            - 4000 제곱피트 이상에 관해서는 데이터 개수가 너무 작아 결론 내리기 어려움
        - 비선형성 이해
            - 원래 큰 집에 500 제곱피트를 추가하는 것보다, 작은 집에 500제곱피트를 추가하는 것이 훨씬 더 큰 차이를 만든다.
                - 비선형 곡선의 기울기를 말하는 거겠지(?)
            - 어쨌든 SqFtTotLiving에 대해 단순선형 항 대신, 비선형항을 고려할 것을 생각해볼 수 있다

## 4.7 다항회귀와 스플라인 회귀

- 응답변수와 예측변수 간의 관계가 비선형성을 띄는 경우 - 회귀모형을 확장

### 4.7.1 다항식

- 다항회귀: 회귀식에 다항 항을 포함한 것
- 편잔차 평활 곡선에 다항회귀 회귀선이 더 가까워졌다

### 4.7.2 스플라인

- 3, 4차 같은 고차 항을 추가하는 것은 회귀방정식에 바람직하지 않은 흔들림을 초래
    
    → 스플라인 - 비선형 관계를 모델링하는 또 다른 방법 
    
- 스플라인: 고정된 점들 사이를 부드럽게 보간하는 방법
    - 일련의 조각별 연속 다항식
    - 구간별 다항식 -예측변수를 위한 일련의 고정된 점(knot, 매듭) 사이를 부드럽게 연결
- R에서 스플라인 설정 파라미터
    - 다항식의 차수 (degress)
    - 매듭의 위치
- 스플라인 항의 계수는 해석하기 어렵다
    - 대신 적합도를 확인하기 위해 시각화 방법을 사용하는 것이 더 유용
    - 편잔차그림을 통해 평활화 곡선과 스플라인 모형이 더 매끄럽게 매칭된다는 것을 확인
        - 그러나 스플라인 회귀가 더 좋다는 것을 의미하는 것은 아님
        - 스플라인 회귀 결과(실선)을 보면 크기가 아주 작은 주택(1000 제곱피트 미만)에서 그보다 크기가 큰 주택보다 더 높은 가치를 가진다고 나왔는데 이는 경제적으로 맞지 않음 - 교란변수 때문일 수 있음 (뭐가 누락된 변수일까? 우편번호 98105인 데이터 한정으로 회귀했는데)

### 4.7.3 일반화가법모형

- 다항 항: 관계를 포착하기에 유연성 부족, 스플라인 항: 매듭을 설정해줘야 함
- GAM(Generalized Additive Model)
    - 스플라인 회귀를 자동으로 찾는 데 사용할 수 있는 유동적 모델링 기술
- [ ]  나중에 더 알아보기

### 4.8 마치며

- 회귀 - 여러 예측변수와 결과 변수 간의 관계를 설정하는 과정
- 기본 형태는 선형 - 각 예측변수는 결과변수와의 선형 관계를 뜻하는 계수를 갖는다
- 다항회귀, 스플라인 회귀 - 일반 회귀 보다 발전된 형태. 비선형 관계도 가능
- 고전적 통계
    - 어떤 현상을 설명, 묘사하기 위해 관측한 데이터에 적합한 모델을 찾는 것을 강조
    - 모델 평가 시 적합도가 얼마나 되는지를 판단하기 위해 기존 ‘표본 내’ 측정 지표를 사용
- 데이터 과학
    - 새 데이터 값을 예측하는 것이 목적
    - 외부 데이터에 대한 예측 정확도를 기반으로 한 지표들 사용
    - 차원을 줄이고 더 컴팩트한 모델을 만들기 위한 변수 선택 방법을 사용