# 2. 데이터와 표본분포

- 용어 정리
    - 분포: 데이터의 값을 크기 순으로 정렬하고 각 값이 몇 번 나오는지를 표시한 것
    - stratum(계층): 공통된 특징을 가진 모집단의 동종 하위 그룹. 복수형은 strata → stratified sampling
    - 표본편향: 모집단과 표본 사이의 차이가 유의미할 만큼 크고, 첫 번째 표본과 동일한 방식으로 추출된 다른 샘플들에서도 이 차이가 계속될 것으로 예상될 때
    - selection bias(선택 편향): 관측 데이터를 선택하는 방식 때문에 생기는 편향
    - data snooping: 뭔가 흥미로운 것을 찾아 광범위하게 데이터를 살피는 것
    - vast search effect: 중복 데이터 모델링이나 너무 많은 예측 변수를 고려하는 모델링에서 비롯되는 편향 혹은 비재현성
    - sample statistic (표본통계량): 더 큰 모집단에서 추출된 표본 데이터들로부터 얻은 측정 지표
    - data distribution: 어떤 데이터 집합에서의 각 개별 값의 도수분포
    - 표본 분포sampling distribution: 여러 표본들 혹은 재표본들로부터 얻은 표본통계량의 도수 분포
    - 중심극한정리 (central limit theorem): 표본 크기가 커질 수록 표본분포가 정규분포를 따르는 경향
    - standard error(표준오차): 여러 표본들로부터 얻은 **표본통계량의 변량**
        - ≠ 표준편차(standard deviation): 개별 데이터 값들의 변량
    - 신뢰수준(confidence level): 같은 모집단으로부터 같은 방식으로 얻은, 관심 통계량을 포함할 것으로 예상되는 신뢰구간의 백분율
    - 오차(error): 데이터 포인트와 예측값(평균) 사이의 차이
    - 정규화하다: 평균을 빼고 표준편차로 나눈다
    - z-score: 개별 데이터 포인트를 정규화한 결과 (정규분포 = z 분포)
    - 표준정규분포: 평균 0, 표준편차 1인 정규분포, x 축의 단위가 평균의 표준편차로 표현
    - qq plot: 표본분포가 특정 분포(예: 정규분포)에 얼마나 가까운지 보여주는 그림
    - 자유도: 다른 표본 크기, 통계량, 그룹의 수에 따라 t 분포를 조절하는 변수
    - 시행(trial): 독립된 결과를 가져오는 하나의 사건
    - 이항식(binomial): 두 가지 결과를 갖는다
    - 이항분포(binomial distribution): n번 시행에서 성공한 횟수에 대한 분포
    - lambda: 단위 시간이나 단위 면적당 사건이 발생하는 비율
    - 푸아송 분포: 표집된 단위 시간 혹은 단위 공간에서 발생한 사건의 도수 분포
    - 지수분포: 한 사건에서 그 다음 사건까지의 (사이) 시간이나 거리에 대한 도수분포
    - 베이불 분포(Weibull distribution): 사건 발생률이 시간에 따라 변화하는, 지수분포의 일반화된 버전

### 2.1 임의표본추출과 표본편향

- 임의표본추출 - 접근 가능한 모집단을 적절히 정의해야
    - 예-고객의 대표 프로파일을 만들 목적으로 파일럿 고객 설문 조사 준비
        - 고객 정의 : 구매 금액이 0보다 큰 모든 고객 명단 → 모든 과거 고객 포함? 제품 환불 고객도 포함? 내부의 테스트 구매자는? 사업자는? 대금 청구 대행사와 고객을 모두 포함?
        - 표본 추출 절차 정의: 무작위로 100명의 고객을 선택 or 유동적인 상황(실시간 거래 고객, 웹 방문자 등)에서는 표본추출 시기가 중요 (시간대 등)
- 층화표본추출 - 모집단을 여러 층으로 나누고, 각 층에서 무작위로 샘플 추출
    - 예-정치 설문 단체에서 백인, 흑인, 라틴계 유권자들의 투표 성향 조사
        - 단순임의표본에서 흑인, 라틴계 인구가 너무 적게 나오는 경우, 해당 층에 높은 가중치를 주는 표본추출 시행
- 빅데이터는 언제 필요할까?
    - 데이터가 크고 동시에 희소할 때
        - 예-구글에서 입력받은 검색 쿼리 데이터 처리
            - column: 용어, row: 개별 검색 쿼리 → 쿼리에 해당 용어가 포함되면 cell 값이 1 (아니면 0)
            - 목표: 주어진 쿼리에 대해 가장 잘 예측된 검색 대상을 결정
            - 영어 단어는 15만 +개, 연간 쿼리는 1조 + 개 → very sparse matrix
            - 방대한 양의 데이터가 누적될 때만 대부분의 쿼리에 대해 효과적인 검색 결과를 반환 가능
                - 데이터가 더 많이 축적될 수록 결과가 더 좋음 → 인기검색어에 대한 검색 결과가 더 좋음
    - 실제 관련이 있는(pertinent, 정확한 검색 쿼리나 아주 비슷한 것이 들어 있는 record, 클릭한 사용자의 정보를 포함) 레코드는 수천 개만 되어도 효과저길 수 있음
        - 그러나 이런 pertinent record를 얻기 위해서는 수조의 데이터 포인트가 필요. 이런 경우는 임의표본추출이 도움되지 않음
- 모평균 vs. 표본평균
    - 표본평균: 관찰을 통해 얻어짐 → 모평균: 작은 표본들로부터 추론

### 2.2 선택편향

- 가지고 있는 데이터에서 패턴 찾기 vs. 가설 수립 → 잘 설계된 실험 수행 → 시험
    - 전자가 참된 패턴인지, 데이터 스누핑을 통해 나온 결과인지 어떻게 알지?
    - 예-동전 던지기
        - 전자: 경기장에 온 2만명에게 동전을 10번 던져서 10번 모두 앞면이 나오면 알려달라고 했다
            - (1 - 아무도 앞면이 10번 나오지 않을 확률)과 같아서, 성공한 사람이 나올 확률이 엄청 높음
            - 그러나 그렇다고 해서 성공한 사람들이 특별한 능력이 있다고 보기는 어려움. 그냥 운
        - 후자: 어떤 사람이 동전을 10번 던져서 10번 모두 앞면이 나오게 할 수 있다고 말했다
            - 한 번 해보라고 했는데(일종의 실험) 실제로 10번 던져서 모두 앞면이 나옴
            - 10번 모두 우연히 앞면이 나올 확률은 1/1024라서 아주 드문 확률 → 그 사람에 특별한 능력이 있다고 생각할 것
- 빅데이터를 반복적으로 조사하는 것
    - 큰 데이터 집합을 가지고 반복적으로 다른 모델을 만들고 다른 질문을 하다보면 언젠가 흥미로운 것을 발견하기 마련 (방대한 검색 효과)→ 정말로 의미 있는 것인가?
    - 성능 검증을 위해…
        - 둘 이상의 holdout set 두기
        - target shuffling (순열검정)
    - 기타 선택 편향 : 비임의 표본추출, 데이터 체리 피킹(썬별), 특정한 통계적 효과를 강조하는 시간 구간 선택, 흥미로운 겨로가가 나올 때 실험을 중단하는 것 등
- 평균으로의 회귀
    - 주어진 어떤 변수를 연속적으로 측정했을 때 나타나는 현상
    - 예외적인 경우가 관찰되면, 그 다음에는 중간 정도의 경우가 관찰되는 경향이 나타남
    
    → 예외 경우를 너무 특별히 생각하고 의미를 부여하면 선택 편향으로 이어질 수 있다 
    

### 2.3 통계학에서의 표본분포

- 표본분포: 하나의 동일한 모집단에서 얻은 여러 샘플에 대한 표본통계량의 분포
    - 고전 통계 대부분은 작은 표본으로 큰 모집단을 추론하는 것과 관련
- sampling variablity 표본의 변동성
    - 다른 표본을 뽑았다면 결과가 얼마나 달라질지

🚨 데이터 분포(개별 데이터 포인트의 분포) vs. 표본분포(표본 통계량의 분포)를 구별하는 것이 중요

- 평균과 같은 표본통계량의 분포
    - 데이터 자체의 분포보다 규칙적이고 종 모양일 가능성이 높다
    - 표본이 클 수록 표본 통계량의 분포가 좁아진다
- 중심극한정리
    - 모집단이 정규분포가 아니더라도
    - 표본크기가 충분하고, 데이터가 정규성(?)을 크게 이탈하지 않는 경우
    - 여러 표본에서 추출한 평균들은 종 모양의 정규곡선을 따른다
    - 효용
        - 추론을 위한 표본분포 (=신뢰 구간, 가설검정 계산)에 t 분포와 같은 정규근사 공식을 사용할 수 있다
    - 하지만 데이터 사이언스에서는 대부분 부트스트랩을 사용할 수 있기 때문에, 중극정의 중요도가 다소간 감소
- **표준오차**
    - 통계에 대한 표본분포의 변동성을 한마디로 말해주는 단일 측정 지표
        - 정의는 여러 표본평균(통계량)의 분포에 대한 표준편차를 의미하지만, 실제 계산에서는 단일 표본만 가지고도 이를 추정할 수 있습니다. 이는 통계 이론에 기반한 공식 덕분입니다.
    - 표본 값들의 표준편차 s, 표본 크기 n을 기반으로 한 통계량을 이용하여 추정
        
        ![Untitled](Untitled%208.png)
        
    - 표준오차 측정 시 고려 사항
        1. 모집단에서 완전히 새로운 샘플들을 많이 수집
            - 그러나 실질적으로 이는 불가능 → 부트스트랩 재표본 사용
        2. 각각의 새 샘플에 대해 통계량(평균)을 계산
        3. 2단계에서 얻은 통계량의 표준편차를 계산 → 표준오차의 추정치로 사용 
    - 부트스트랩은 표준오차를 추정하는 표준 방법

### 2.4 부트스트랩

- 부트스트랩
    - 표본통계량의 변동성을 평가하는 강력한 도구
    - 현재 있는 표본에서 추가적으로 표본을 복원 추출하고 각 표본에 대한 통계량과 모델을 다시 계산
    - 데이터나 표본통계량이 정규분포를 따라야 한다는 가정은 필요하지 않음
    - 복원추출
        - 표본을 뽑은 후 각 관측치를 다시 원래 자리에 돌려놓는다
        
        → 뽑을 때마다 각 원소가 뽑힐 확률은 그대로 유지하면서 무한한 크기의 모집단을 생성 가능 
        
    - 칼럼이 여러 개인 다변량 데이터에 대해서도 부트스트랩 적용 가능
        - 각 행이 하나의 샘플이 됨
    - 모집단에서 추가적으로 표본을 뽑는다고 할 대, 그 표본이 얼마나 원래 (갖고 있는) 표본과 비슷할지를 알려줌
- 크기 n의 샘플의 평균을 구하는 부트스트랩 재표본 추출 알고리즘
    1. 샘플 값을 하나 뽑아서 기록하고 다시 제자리에 놓는다 
    2. n번 반복한다 ← 갖고 있는 데이터 (크게 n의 샘플)과 동일한 크기로 추출
    3. 재표본추출된 값의 평균을 기록한다 
    4. 1~3을 R번 반복한다
    5. R개의 결과(표본 통계량)을 사용하여 
        1. 표준오차(표본평균의 표준편차)를 계산
        2. 히스토그램 또는 상자그림을 그린다
        3. 신뢰구간을 찾는다  
- 부트스트랩 결과 해석
    
    ![Untitled](Untitled%209.png)
    
    - median 원래 추정치(갖고 있는 데이터로만 구한 수치): 62000
    - 부트스트랩 분포는 추정치에서 약 -81달러만큼의 편향이 있음
        - 부트스트랩 추정치 평균(61918.015) - 원래 추정치(62000) = -81.985
    - 표준오차: 229.6991
    - python 코드로 보면 구하는 과정이 더 명확
        
        ```python
        results = []
        for n_repeat in range(1000):
            sample = resample(loans_income)
            results.append(sample.median())
        results = pd.Series(results)
        print('Bootstrap Statistics: ')
        print(f'original: {loans_income.median()}')
        print(f'bias: {results.mean() - loans_income.median()}')
        print(f'std. error: {results.std()}')
        ```
        
- 배깅
    - 분류 및 회귀 트리를 사용할 때
    - 여러 부트스트랩 샘플을 가지고 트리를 여러 개 만든 다음
    - 각 트리에서 나온 예측값을 평균 내는 것
- 재표본추출(resample) vs. 부트스트랩핑
    - resample: 여러 표본이 결합되어 비복원 추출을 수행할 수 있는 순열 과정을 포함한다
    

### 2.5 신뢰구간

- 어떤 단일 수치가 제시될 때(점추정), 추정치에 과도한 믿음을 둔다
    
    → 단일 수치가 아닌 어떤 범위로 추정치를 제시하는 것이 이러한 경향을 막는 방법 
    
- 신뢰구간은 백분율로 표현되는 포함 수준과 함께 나온다
    - 구간 범위로 추정값을 표시하는 일반적인 방법
    - 90% 신뢰구간: 표본통계량의 부트스트랩 표본분포의 90%를 포함하는 구간
        - 예) 부트스트랩으로 1000개의 평균을 구한 뒤, 크기 순으로 정렬했을 때, 가장 작은 50개, 가장 큰 50개 총 100개를 제하고, 나머지 900개를 포함하는 구간
- 부트스트랩 신뢰구간 구하는 법
    1. 데이터에서 복원 추출 방식으로 크기 n인 표본을 뽑는다(재표본추출)
    2. 재표본추출한 표본에 대해 원하는 통계량을 기록한다 - 평균, 중간값 등 
    3. 1~2를 R번 반복한다 → R개의 통계량 얻음
    4. x% 신뢰구간을 구하기 위해, R개의 재표본 결과의 분포 양쪽 끝에서 [(100-x) / 2] %만큼 잘라낸다 
        - 예) 100-90 / 2 = 5% → 1000개 * 0.05 = 50개
    5. 절단한 점들은 x% 부트스트랩 신뢰구간의 양 끝 점 
- 신뢰 수준이 높을 수록 구간이 더 넓어진다
    - 구간 안에 포함해야 하는 값이 늘어나기 때문에
- 표본이 작을 수록 구간이 넓어진다
    - 표본이 작으면 표본 통계량이 들쭉 날쭉(불확실성 커짐) → 모든 표본 통계량을 포함하려면 구간이 넓어야 한다

### 2.6 정규분포

- qq plot
    - y축: z 점수를 정렬하고, 각 값의 z 점수를 y축에 표시
    - x축: 정규(특정)분포에서의 해당 분위수
        - 데이터가 표준화되었기 대문에 단위는 평균으로부터 떨어진 데이터의 표준편차 수
        - x축 값 구하는 방법
            
            1. 데이터 크기 확인: 데이터셋의 크기를 n으로 설정합니다.
            
            2. 분위수 계산: 데이터셋의 각 데이터 포인트에 대해 분위수를 계산합니다. 일반적으로 이는 데이터의 순서에 따라 1/(n+1), 2/(n+1), ..., n/(n+1)의 확률 값을 할당합니다.
            
            3. 이론적 정규분포에서 분위수 값 찾기: 각 분위수 값에 대해 이론적 정규분포(평균 0, 표준편차 1)의 값을 계산합니다. 이 계산은 qnorm 함수를 사용하여 수행됩니다.
            
            - `theoretical_quantiles <- qnorm(probabilities, mean = 0, sd = 1)`
                - 첫번째 파라미터: 분위수에 해당하는 확률 값입니다
                    - 예를 들어, 0.25는 제1사분위수에 해당
                    - probabilities: (1:n) / (n + 1) → qnorm → 표준정규분포에서 각 순위(?)에 해당하는 수를 얻을 수 있음
            

### 2.7 긴 꼬리 분포

- 분포의 꼬리: 양쪽의 극한 값
- skewness (왜도): 분포의 한쪽 고리가 반대쪽 다른 꼬리보다 긴 정도
- `nflx <- diff(log(nflx[nflx>0]))`
    1. `nflx[nflx > 0]`:
        - nflx 데이터에서 값이 0보다 큰 항목만 선택합니다. 주식 가격 데이터에서 0 이하의 값은 존재하지 않아야 하기 때문에, 이는 데이터 정제의 일환일 수 있습니다.
    2. `log(nflx[nflx > 0])`:
        - 선택된 주식 가격에 자연로그를 적용합니다. 로그 변환은 데이터의 분포를 정규화하는 데 유용하며, 특히 주식 가격 데이터에서 로그 수익률을 계산하는 데 일반적으로 사용됩니다.
            - 로그 수익률의 이점
                
                로그 수익률을 사용하는 데는 여러 가지 이유와 장점이 있습니다. 주요 장점은 다음과 같습니다:
                
                ### 1. **연속 복리 계산**
                
                - 로그 수익률은 연속 복리를 자연스럽게 표현합니다. 즉, 여러 기간의 로그 수익률을 더하면 전체 기간의 로그 수익률을 구할 수 있습니다.
                - 예를 들어, \(r_1\), \(r_2\), \(r_3\)가 각 기간의 로그 수익률이라면, 전체 기간의 로그 수익률은 \(r_1 + r_2 + r_3\)입니다.
                
                ### 2. **대칭성**
                
                - 로그 수익률은 대칭적입니다. 즉, 수익률이 -100%가 되는 것은 값이 0으로 수렴하는 것을 의미하며, 수익률이 +100%가 되는 것은 값이 두 배가 되는 것을 의미합니다.
                - 비율 수익률에서는 +100%와 -100%의 변화가 비대칭적입니다. 예를 들어, 50% 상승한 후 다시 50% 하락하면 원래 값으로 돌아오지 않습니다. 그러나 로그 수익률에서는 이러한 비대칭성이 없습니다.
                
                ### 3. **분포의 정상화**
                
                - 로그 변환을 통해 수익률 분포를 더 정규화할 수 있습니다. 이는 정규분포를 가정하는 통계 기법들을 적용하기 쉽게 만듭니다.
                - 주식 가격의 비율 수익률은 종종 정규분포를 따르지 않지만, 로그 수익률은 더 정규분포에 가까운 경향이 있습니다.
                
                ### 4. **누적 수익률 계산의 용이성**
                
                - 여러 기간의 로그 수익률을 합산하면 총 누적 수익률을 쉽게 계산할 수 있습니다.
                - 비율 수익률에서는 복잡한 곱셈을 사용해야 하지만, 로그 수익률에서는 단순히 덧셈을 사용합니다.
                
                ### 예시
                
                1. **비율 수익률**:
                    - 첫 번째 기간의 비율 수익률이 10%이면: \((P_1 - P_0) / P_0 = 0.10\)
                    - 두 번째 기간의 비율 수익률이 5%이면: \((P_2 - P_1) / P_1 = 0.05\)
                    - 전체 기간의 비율 수익률은: \((1 + 0.10) \times (1 + 0.05) - 1 = 0.155 = 15.5%\)
                2. **로그 수익률**:
                    - 첫 번째 기간의 로그 수익률이 10%이면: \(\log(P_1 / P_0) = \log(1.10) = 0.0953\)
                    - 두 번째 기간의 로그 수익률이 5%이면: \(\log(P_2 / P_1) = \log(1.05) = 0.0488\)
                    - 전체 기간의 로그 수익률은: \(0.0953 + 0.0488 = 0.1441 = \log(1.155)\)
                    - 따라서, 전체 기간의 비율 수익률은 \(e^{0.1441} - 1 = 0.155 = 15.5%\)
                
                ### 요약
                
                로그 수익률은 연속 복리 계산, 대칭성, 분포의 정상화, 누적 수익률 계산의 용이성 등의 장점 때문에 금융 및 경제 분석에서 많이 사용됩니다. 이를 통해 더 정확하고 해석하기 쉬운 수익률 분석이 가능합니다.
                
    3. `diff(log(nflx[nflx > 0]))`:
        - 로그 변환된 주식 가격의 차분을 계산합니다.
            - 차분은 시계열 데이터의 인접한 값들 사이의 차이를 계산하는 방법입니다. 이를 통해 데이터를 안정화하고 추세를 제거하며, 시계열 분석 및 예측 모델링에서 유용하게 사용됩니다.
        - diff 함수는 시계열 데이터에서 인접한 값들의 차이를 계산합니다. 이는 로그 수익률을 의미
            - 로그 수익률: log(현재 가격) - log(이전 가격)로 계산되며, 이는 log ** {현재 가격- 이전 가격}과 동일합니다.
- qq plot 해석
    
    ![Untitled](Untitled%2010.png)
    
    - 낮은 값의 점들은 대각선보다 훨씬 낮고, 높은 값은 대각선보다 훨씬 높음
        
        → 데이터가 정규분포를 따르지 않는다는 것을 의미
        
        → 정규분포보다 훨씬 더 많은 극단값을 관찰할 가능성 
        
    - 평균에서 표준편차 이내(x축에서 -1~1)에 있는 데이터의 점들은 선에 가까이 있다
        
        → 중간에서는 정상이면서 긴 꼬리를 갖는 현상 
        
    

### 2.8 스튜던트의 t 분포

- 정규분포와 비슷하게 생겼지만 꼬리 부분이 약간 더 두껍고 길다
- 표준화된 여러 통계 자료를 t분포와 비교하여 신뢰구간을 추정할 수 있다
    - 예-표본평균이 $\bar{x}$, 크기가 n인 표본 → s가 표본표준편차
        - 표본평균 주위의 90% 신뢰구간 : $\bar{x}\pm \textit{t}_{n-1}(0.05)\cdot {s\over{\sqrt{n}}}$
        - $\textit{t}_{n-1}(0.05)$ : (n-1) 자유도를 갖는 t 분포의 양쪽 끝에서 5%를 잘라내는 t 통계량
- t 분포 사용 목적
    - 표본평균, 두 표본평균 간의 차이, 회귀 파라미터, 그 외 다른 통계량들의 분포를 구할 때 사용
- t 분포의 정확도는 표본에 대한 통계량의 분포가 정규분포를 따른다는 조건을 필요로 함
    
    → 중심극한정리에서 만족됨
    
- 그러나 사실 데이터 사이언스에서 t분포를 깊게 알아야 할 것은 없다…우리에게 중요한 것은 불확실성과 변동성

### 2.9 이항분포

- binary 의사 결정 과정에서 아주 중요 - 시행: 정해진 확률로 두 가지 결과를 가짐
- 이항분포: 각 시행마다 그 성공 확률이 정해져 있을 때, 주어진 시행 횟수 중에서 성공한 횟수의 도수분포
    - 예-한 번의 클릭이 판매로 이어질 확률이 0.02(`p`)일 때, 200회(`size`) 클릭으로 0(`x`)회 매출을 관찰할 확률은 얼마인가?
    - 평균: n*p, 분산: n*p*(1-p)
- n이 크고, p가 0 또는 1에 너무 가깝지 않은 경우, 이항분포는 정규분포로 근사할 수 있다

### 2.10 카이제곱분포

- 번주에 속하는 주제, 항목의 수와 관련
- 카이제곱검정에서의 기댓값
    - 귀무가설(null hypothesis)이 참일 때, 각 범주에 대해 기대되는 관측 빈도
    - 데이터에서 특이하거나 주목할 만한 것이 없다 (변수나 패턴 사이에 상관 관계가 없음)
- 카이제곱통계량
    - 관측된 데이터가 귀무가설(즉, 두 변수 사이에 관계가 없다는 가정)에서 기대되는 값과 얼마나 다른지(벗어나있는지) 를 계산하는 값
    - 계산
        
        ![Untitled](Untitled%2011.png)
        
        - 관측값과 기댓값의 차이를 제곱한 다음, 기댓값으로 나눈 결과를 모든 카테고리에 대해 합산한 값
    - 적합도 검정에 사용됨
        - 관측 데이터가 특정 분포에 적합한 정도를 나타냄
        - 여러 처리의 효과가 서로 다른지 여부를 결정하는데 유용
- 카이제곱분포
    - 귀무 모델에서 반복적으로 재표본추출한 통계량 분포
    - 개수 집합에 대해 카이제곱 값이 낮다 = 기대 분포를 거의 따르고 있다는 것을 의미

### 2.11 F분포

- 수치로 측정된 데이터와 관련된 실험 및 선형 모델
- 여러 그룹에 걸쳐 서로 다른 처리를 테스트
    - 예-밭의 구역별로 다른 비료를 사용
    - 카이제곱분포의 A/B/C 검정과 유사하지만, 횟수(frequency)가 아닌 연속된 관측값을 처리
    - 그룹 평균 수치 간의 차이가 정규 무작위 변동에서 예상할 수 있는 것보다 얼마나 큰지에 관심
- F 통계량
    - 각 그룹내 변동성에 대한 / 그룹 평균 간 변동성의 비율을 의미
        - 그룹 내부의 데이터 변동과 그룹 간의 평균 차이를 비교한 비율
    - ANOVA (ANalysis Of VAriance)
    - 귀무모델(모든 그룹의 평균이 동일한 경우) 무작위 순열 데이터에 의해 생성되는 모든 값의 빈도 분포
    

### 2.12 푸아송 분포와 그 외 관련 분포들

- 많은 작업이 주어진 어떤 비율(람다)에 따라 임의로 사건을 발생 시킨다
    - 예-웹사이트 방문객 수, 톨게이트에 들어오는 자동차(시간에 따른 사건), 1제곱미터 당 건물의 결함, 코드 100줄 당 오타(공간에 따른 사건)
- 푸아송 분포
    - 이전 집계 데이터를 통해 시간 단위 혹은 공간 단위에서의 평균적인 사건의 수 추정
        - 예-연간 독감 감염자 수 → 일일 감염 혹은 인구 조사 단위당 감염 수 추정
    - 시간 단위 또는 공간 단위로 표본을 수집할 때 그 사건들의 분포를 알려준다
        - 예-5초 동안 서버에 도착한 인터넷 트래픽을 95%의 확률로 완벽하게 처리하는 데 필요한 용량은 얼마일까? 같은 대기행렬 관련 질문을 처리할 때 유용
    - 평균과 분산 모두 람다
    - 예-고객 서비스 센터에서 1분당 평균 2회로 문의 전화가 접수된다면, 100분을 시뮬레이션하여 100분당 문의 전화 횟수는? → 1분당 총 몇 번씩 문의 전화가 오는지 array로 보여줌
        - `stats.poisson.rvs(2, size=100)`
- 지수분포
    - 동일한 변수 람다를 사용하여 사건과 사건 간의 시간 분포를 모델링
        - 예-웹사이트 방문이 일어난 시간, 톨게이트에 자동차가 도착하는 시간 **사이**, 고장이 발생하는 시간을 모델링, 프로세스 관리에서는 개별 고객 상담에 소요되는 시간을 모델링하는 데도 사용
    - random variates 생성을 위해서는 푸아송 분포에서처럼 생성할 난수 개수랑, 비율(시간 주기당 사건수-람다) 사용
    - 예-고객 서비스 센터에서 분당 평균 0.2회 서비스 문의 전화가 오는 경우, 100분 동안의 서비스 센터 문의 전화를 시뮬레이션 가능
        - `stats.expon.rvs(scale=1/0.2, size=100)`
    - 평균은 1/람다 → 람다 = 1/스케일
        - 스케일: 지수분포의 평균
- 푸아송, 지수 분포 모두 람다가 해당 기간 동안 일정하게 유지된다는 가정
    - 현실적으로 힘들지만, 시간 주기 또는 공간을 일정 기간 충분히 동일하도록 영역을 잘 나누면 시뮬레이션 가능

### 2.13 고장률 추정

- 드물게 발생하는 사건의 경우 - 사건 발생 비율 람다를 사전에 구하기 힘듦
    - 예-항공기 엔진 고장
        - 고장이 안나면 사건 발생률 추정할 근거가 전무
        - 그러나 추측 가능 - 20시간 후에도 아무일도 일어나지 않았따면, 시간당 발생률이 1이 아니라는 것은 분명히 알 수 있다
- 시뮬레이션이나 확률의 직접 계산 → 다른 가상 사건 발생률 평가 → 그 이하로 떨어지지 않을 임계값을 추산 가능
- 데이터가 있어도 신뢰할만한 발생률을 추정하기에 충분하지 않은 경우, 적합도 검정을 통해 적용한 여러 발생률 중 어느 것이 관찰된 데이터에 가장 적합한지를 알 수 있다

### 2.14 베이불 분포

- 사건 발생률은 시간에 따라 일정하지 않다
    - 변화 주기가 일반적인 사건 발생 구간 보다 길다면 괜찮음
        - 비율이 상대적으로 일정한 구간으로 분석을 세분화하면 되니까
    - 사건 발생률이 시간에 따라 지속적으로 변한다면 지수, 푸아송 분포는 효용 X
        - 예-기계 고장. 시간이 지날 수록 고장 위험 증가
- 베이불 분포 (와이블 분포)
    - 지수 분포의 확장
    - shape parameter beta로 지정된 대로 발생률이 달라질 수 있음
        - beta > 1 → 발생률이 시간이 지나면서 증가
        - beta < 1 → 발생률이 시간이 지나면서 감소
    - 고장 시간 분서에 주로 사용되어 두번째 parameter는 구간 당 사건 발생률 보다, 특성 수명으로 표현 ($\eta$, eta), scale 변수라고도 함
    - 베이불 분포 사용 = 두 파라미터 beta, eta의 추정이 포함
    - 예- 1.5 shape, 5000의 특성 수명을 갖는 베이불 분포에서 수명 100개 생성
        - 개별 난수 해석: 특정 개체의 수명이나 특정 사건이 발생하기까지의 시간
            - 예-생성된 난수가 4500이라면, 이는 해당 개체가 4500 시간(또는 다른 단위) 동안 생존할 것으로 예상된다는 의미입니다.