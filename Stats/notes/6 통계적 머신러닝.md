# 6. 통계적 머신러닝

- 용어 정리
    - neighbor: 예측변수(X)에서 값들이 유사한 레코드(행)
    - distance metric: 각 레코드 사이가 얼마나 멀리 떨어져 있는지를 나타내는 단일 값
    - 표준화(standardization, 정규화): 평균을 뺀 후에 표준편차로 나누는 일
    - z score: 표준화를 통해 얻은 값
    - k: 최근접 이웃을 계산하는 데 사용되는 이웃의 개수
    - 재귀 분할(recursive partitioning): 마지막 분할 영역에 해당하는 출력이 최대한 비슷한(homogeneous) 결과를 보이도록 데이터를 반복적으로 분할하는 것
    - 분할값(split value): 분할값을 기준으로 예측변수를 그 값보다 작은 영역과 큰 영역으로 나눈다
    - 마디(node): 의사 결정 트리와 같은 가지치기 형태로 구성된 규칙들의 집합에서, 노드는 분할 규칙의 시각적인 표시라고 할 수 있다
    - 잎(leaf): if-then 규칙의 가장 마지막 부분, 혹은 트리의 마지막 가지(branch) 부분을 의미. 트리 모델에서 잎 노드는 어떤 레코드에 적용할 최종적인 분류 규칙을 의미
    - 손실(loss): 분류 과정에서 발생하는 오분류의 수. 손실이 클 수록 불순도가 높다고 할 수 있음
    - 불순도(impurity, heterogeneity): 데이터를 분할한 집합에서 서로 다른 클래스의 데이터가 얼마나 섞여 있는지를 나타냄. 더 많이 섞여 있을 수록 불순도가 높음 ↔ 동질성(homongeneity), 순도
    - 가지치기(pruning): 학습이 끝난 트리 모델에서 오버피팅을 줄이기 위해 가지들을 하나씩 잘라내는 과정
    - 앙상블(ensemble, model averaging): 여러 모델의 집합을 이용해서 하나의 예측을 이끌어내는 방식
    - 배깅(bagging, bootstrap aggregating): 데이터를 부트스트래핑해서 여러 모델을 만드는 일반적인 방법
    - 랜덤 포레스트(random forest): 의사 결정 트리 모델에 기반을 둔 배깅 추정 모델
    - 변수 중요도(variable importance): 모델 성능에 미치는 예측 변수의 중요도
    - 앙상블: 여러 모델들의 집합을 통해 예측 결과를 만들어내는 것
    - 부스팅: 연속된 라운드마다 잔차가 큰 레코드들에 가중치를 높여 일련의 모델들을 생성하는 일반 기법
    - AdaBoost: 잔차에 따라 데이터의 가중치를 조절하는 부스팅의 초기 버전
    - Gradient boosting: cost function을 최소화하는 방향으로 부스팅을 활용하는 일반적인 형태
    - stochastic gradient boosting: 각 라운드마다 레코드와 열을 재표본 추출하는 것을 포함하는 부스팅의 가장 일반적인 형태
    - regularization: 비용함수에 모델의 파라미터 개수에 해당하는 벌점 항을 추가해 오버피팅을 피하는 방법
    - hyperparameter: 알고리즘을 피팅하기 전에 미리 세팅해야 하는 파라미터

---

- 주요 개념
    - KNN 방법이란 유사한 레코드들이 속한 클래스로 레코드를 분류하는 방법
    - 유사성(거리)는 유클리드 거리나 다른 관련 지표들을 이용해 결정
    - 가장 가까운 이웃 데이터의 개수를 의미하는 k는 학습 데이터에서 얼마나 좋은 성능을 보이는지를 가지고 결정
    - 일반적으로 예측 변수들을 표준화. 스케일이 큰 변수들의 영향력이 너무 커지지 않도록
    - 예측 모델링의 첫 단계에서 종종 KNN 사용. 이렇게 얻은 값을 다시 데이터에 하나의 예측 변수로 추가해서 두번째 단계의 (KNN이 아닌) 모델링을 위해 사용
    - 의사 결정 트리는 결과를 분류하거나 예측하기 위한 일련의 규칙 생성
    - 이 규칙들은 데이터를 하위 영역으로 연속적으로 분할하는 것과 관련
    - 각 분할 혹은 분기는 어떤 한 예측변수 값을 기준으로 데이터를 위 아래 두 부분으로 나눈 것
    - 각 단계마다 트리 알고리즘은 결과 불순도를 최소화하는 쪽으로 영역 분할 진행
    - 더 이상 분할이 불가능할 때 트리가 완전히 자랐다고 볼 수 있으며 각 말단 노드 혹은 잎 노드에 해당하는 레코드들은 단일 클래스에 속함. 새로운 데이터는 이 규칙 경로를 따라 특정 클래스로 할당
    - 완전히 자란 트리는 데이터를 오버피팅하기 때문에, 노이즈를 제외한 신호에만 반응하도록 트리에 가지치기를 수행해야
    - 랜덤 포레스트나 부스팅 트리 같은 다중 트리 알고리즘은 우수한 예측 성능 보장.하지만 규칙에 기반을 둔 단일 트리 방법의 장점이라고 할 수 있는 전달 능력을 상실
    - 앙상블 모델은 많은 모델로부터 얻은 결과를 서로 결합해 모델 정확도를 높인다
    - 배깅은 앙상블 모델 가운데 하나의 형태로, 부트스트랩 샘플을 이용해 많은 모델들을 생성하고 이 모델들을 평균화한다
    - 랜덤 포레스트는 배깅 기법을 의사 결정 트리 알고리즘에 적용한 특별한 형태. 랜덤 포레스트에서는 데이터를 재표본추출하는 동시에 트리를 분할할 때 예측변수 또한 샘플링
    - 랜덤 포레스트로부터 나오는 출력 중 유용한 것은 예측변수들이 모델 정확도에 미치는 영향력을 의미하는 변수 중요도
    - 랜덤 포레스트에서는 오버피팅을 피하기 위해 교차타당성검사를 통해 조정된 하이퍼파라미터를 사용

---

### 6.0 개요

- 지도 학습 방법
    - 결과 알려진 데이터를 가지고 훈련한 후 새로운 데이터에 대한 결과를 예측
    - 통계적 머신러닝에 속함
    - 데이터에 기반하며 전체적인 구조(모델이 선형인지 등)을 가정하지 않는다는 점에서 고전적인 통계 방법과 구별
        - c.f. ML vs. Stat
            - ML
                - 예측 모델을 최적화하기 위해 많은 데이터를 효과적으로 처리하는 알고리즘 개발 집중
            - Stat
                - 확률론과 모델의 구조를 결정하는데 관심
- 앙상블 학습
    - 최종 예측을 얻기 위해 많은 모델 사용
- 의사 결정 트리 (Decision Tree)
    - 예측변수와 결과변수 사이의 관계 규칙을 학습하는 유연하고 자동화된 기술
- 앙상블 + Decision Tree는 성능 좋은 예측 모델링 기법

## 6.1 KNN (K-nearest neighbors)

- 메커니즘
    1. 특징들이 가장 유사한(X들이 유사한) k개의 레코드를 찾는다
    2. 분류: 이 유사한 레코드들 중에 다수가 속한 클래스가 무엇인지 찾고, 새로운 레코드를 그 클래스에 할당
    3. 예측(KNN regression): 유사한 레코드들의 평균을 찾아서 새로운 레코드에 대한 예측값으로 사용
- 회귀와 달리 모델 피팅 필요 없음. 그러나 완전히 자동화된 방법은 아님
    - 특징들이 어떤 척도에 존재하는지
    - 가까운 정도를 어떻게 측정할 것인지
    - k를 어떻게 설정할 것인지에 따라 예측 결과 차이
- 모든 예측 변수들은 수치형이어야 함

### 6.1.1 예제: 대출 연체 예측

- 그래프
    
    ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled.png)
    
    - 가장 가까운 20개 점들에 대한 경계선 (이웃 중에 max distance를 반지름으로 삼아서)
    - k를 몇 개로 잡느냐에 따라 분류 결과가 달라짐
        - 가장 가까운 3개 기준으로 하면 default vs. 20개 기준으로 하면 paid off
- KNN 가지고 경향 점수 얻기
    - k개의 가장 가까운 점들이 속한 클래스의 비율로 확률 계산
    - 예-20개 중에 연체인 이웃의 개수는 9 → 연체일 가능성은 9/20*100 =. 5%
    - 희귀한 사건에 대한 확률로 컷 오프를 정하는 것이 일반적인 방법
        - 보통 50%미만

### 6.1.2 거리 지표

- 거리 지표를 통해 결정되는 유사성(근접성)
    - 두 데이터가 얼마나 서로 멀리 떨어져 있는지 측정
    - 유클리드, 맨해튼은 각 예측변수가 독립적이라고 가정
- 유클리드 거리
    - target record와 다른 모든 record 사이에 아래의 작업 수행
        - 모든 예측변수(X 칼럼) 별 차이 값을 구한 뒤, 그 값을 제곱해서 다 더한 다음, 루트를 씌운다
    - k * n (다른 record 개수) 번만큼 pairwise 비교 필요
    - 두 점 사이의 직선 거리
- 맨해튼 거리
    - 모든 예측변수별 차이 값을 구한 뒤 절대값 취해서 다 더하기
    - 한 번에 대각선이 아닌 한 축 방향으로만 움직일 수 있다고 할 때, 두 점 사이의 거리
        - 그림
            
            ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%201.png)
            
- 예측 변수 별로 스케일이 다른 경우, 큰 스케일에서 측정된 변수들이 거리에 미치는 영향이 크다
    - 데이터 표준화를 통해 스케일이 작은 변수도 동등한 영향력을 가지도록 해줄 수 있음
- 마할라노비스 거리
    - 수치 데이터에 대해 두 변수 간의 상관관계 사용
    - 두 변수 사이의 높은 상관관계가 있으면 유용
    - 유클리드나 맨해튼에서 상관성이 있는 변수들에서 원인이 되는 속성에 더 큰 가중치 부여
        - 변수들이 상관 관계가 있는 경우, 실제로는 함께 변동하는 경향이 있음에도 불구하고, 유클리드 거리와 맨해튼 거리는 각각의 변수 차이에 독립적으로 가중치를 부여
        - 그 결과 상관된 변수들에서 특정 변수의 차이가 거리 계산에 불필요하게 큰 영향을 미칠 수 있음
        - 예-키가 큰 사람은 몸무게도 많이 나갈 가능성이 높음
            - 유클리드 거리나 맨해튼 거리에서는 키와 몸무게를 독립적으로 고려하기 때문에, 키나 몸무게 중 하나에서 큰 차이가 발생하면 그것이 전체 거리 계산에 큰 영향
            - 을 미칩니다. 이는 실제 상관 관계를 반영하지 않는 왜곡된 거리 계산을 초래할 수 있습니다.
    - 반대로 마할라노비스 거리에서는 상관성 있는 변수들의 가중치가 감소
        - 공분산 행렬의 역행렬을 사용하여 각 변수의 기여도를 조정
            - 역행렬은 공분산 값이 큰 예측변수들의 영향을 감소시키는 역할
                
                ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%202.png)
                
        - 공분산 값이 큰 예측변수들은 마할라노비스 거리 계산 시 그 영향이 감소
    - 공분산 행렬 사용하기 때문에 계산량 커지고 복잡성 증가

### 6.1.3 원-핫 인코더

- factor column 값들을 개별 요인으로 분리한 뒤, 0/1 표현해야 함
    - 개별 요인 개수만큼 칼럼 추가됨
- 선형회귀나 로지스틱 회귀에서는 원-핫 인코딩 하면 다중공선성 관련 문제 발생
    - 한 요인은 생략하고, 다른 값들로부터 유추 (같은 요인 카럼에서 파생된 다른  binary 칼럼 값들이모두 0이면 해당 요인이 1이기 때문에)
    - KNN에서는 문제 아님

### 6.1.4 표준화(정규화, z점수)

- 일반적으로 정규화는 X(train data columns)에 대해 수행
- 값이 절대적으로 얼마인지보다, 평균과 상대적으로 얼마나 차이가 나는지 관심 있을 때
- 표준화(정규화)
    - 모든 변수에서 평균을 빼고 표준편차로 나눠서 변수들을 모두 비슷한 스케일에 놓는다
    - 실제 측정된 값의 스케일 때문에 모델에 과도하게 큰 영향을 주는 것을 막을 수 있음
    - $z = \frac{x-\bar{x}}{s}$
        - 평균으로부터 표준편차만큼 얼마나 떨어져 있는지
- KNN이나 주성분분석, 클러스터링과 같은 알고리즘에서는 데이터를 미리 표준화하는 것이 필수
- 정규화 전후 결과 비교
    - 전: revol_bal 칼럼 빼고 다른 칼럼들 값은 target(newloan)으로부터 넓게 퍼져 있음
        - 가장 스케일이 큰 revol_bal의 영향이 가장 크기 때문에
        
        ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%203.png)
        
    - 후: 새롭게 얻은 최근접 이웃들은 모든 변수에서 훨씬 더 유사
        - 변수 스케일에 영향 받지 않고 모든 변수의 영향력이 비슷해졌기 때문
            - 정규화 이후 트레이닝 데이터의 모습 (스케일 차이 없어짐)
                
                ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%204.png)
                
        
        ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%205.png)
        
- z점수만 있는 건 아니고…
    - 평균 대신 중간값과 같은 좀 더 로버스트한 위치 추정값을 사용할 수도 있음
    - 표준편차 대신에 사분위범위와 같은 다른 척도 추정 방법 사용 가능
    - 단위 분산을(분산1)을 갖도록 모든 변수를 조정하는 것이 늘 자연스러운 것도 아님
        - 각 변수가 예측력 측면에서 갖는 중요성이 누가봐도 서로 다른 경우
- 정규화 자체는 데이터 분포에 영향을 주지 않음
    - 정규화한다고 정규분포가 되는 건 절대 아님

### 6.1.5 k 선택하기

- 최적의 k값 찾기
    - k가 너무 작으면 데이터의 노이즈 성분까지 고려하는 오버피팅 문제 발생
        - k=1
            - target data와 가장 가까운 한 행을 찾아서 예측 결과로 사용
    - k가 너무 크면 결정함수가 너무 과하게 평탄화되어(오버스무팅) 데이터의 지엽적인 정보를 예측하는 KNN의 기능 상실
    - 홀드 아웃 데이터에서의 정확도 지표 확인
    - 노이즈가 거의 없고 잘 구조화된 데이터
        - k값이 작을 수록 잘 동작
        - 신호 대 잡음비 (signal noise ratio, SNR)가 높은 데이터
        - 손글씨, 음성인식 데이터 등
    - 노이즈가 많아 SNR이 낮은 데이터
        - 대출 데이터 등
        - k값이 클수록 좋음. 1~20 사이 보통
    - 동률이 나오는 경우를 막기 위해 보통 K는 홀수
- Bias-Variance trade off
    - 통계적 모델 적합화에서 늘 존재하는 문제
        - 과대 평탄화(oversmoothing) vs. 과대 적합화(overfitting)
    - 분산
        - 학습 데이터 선정에 따라 발생하는 모델링 오차
        - 다른 학습 데이터 집합을 사용할 경우, 결과적으로 나오는 모델이 달라지는 정도
    - 편향
        - 모델이 실제 세계를 정확히 표현하지 못하기 때문에 발생하는 모델링 오차
        - 모델에 데이터를 추가한다고 해서 나아지는 것이 아님
    - 유연한 모델에서 오버피팅 발생 → 분산 증가 의미
        - 단순한 모델 사용? 모델이 실제 문제를 정확히 표현하지 못할 정도로 잃어버린다면 편향이 증가할 수도
    - 교차타당성 검사를 통해 trade off를 따져본다

### 6.1.6 KNN을 통한 피처 엔지니어링

- 분류 성능 자체가 경쟁력 있기보다는, 다른 분류 방법들의 특정 단계에 도움
- 모델에 local knowledge 추가를 위한 KNN 사용
    1. 데이터에 기반해서 KNN으로 target data가 특정 클래스에 속할 확률을 얻는다 
    2. 이 결과를 해당 record의 새로운 feature(칼럼값)으로 추가
    3. 이 결과까지 합쳐서 다른 분류 방법에 예측 변수로 사용한다 
        - 예측 변수로부터 하나의 예측 변수가 파생되지만 다중공선성 문제는 X
        - 소수의 근접한 레코드들로부터 얻은 매우 지엽적인 정보이기 때문
- 예-킹 카운티 주택 데이터
    - 주택 가격 산정 시 최근에 팔린 비슷한 집들의 가격을 기준으로 삼음
        - 비슷한 주택 매매 가격을 일일이 확인하면서 일종의 수동식 KNN
    - KNN을 사용한다고 하면, 추가되는 새로운 예측 변수는 각 레코드에 대한 KNN 예측변수
        - 예측 결과는 클래스가 아니라 수치형 변수
        - 다수결 결과가 아닌 k-최근접 이웃값의 평균사용 (KNN 회귀)

## 6.2 트리 모델 🎄

- CART(classification and regression tree), decision tree
- 쉽게 말해 if-then-else 규칙의 집합체
- 선형회귀, 로지스틱은 주어진 모델을 데이터에 맞게 fitting ↔ 트리는 데이터에서 패턴을 발견
- c.f. 운용과학에서의 decision tree
    - 인간의 의사 결정 과정에 대해 연구하는 운용 과학 분야에서 decision tree는 다른 의미
    - 분기 다이어그램에 나타난 결정 지점, 가능한 결과와 예상 확률 등이 주어진 상태에서, 최대 기댓값을 갖는 의사 결정 경로를 선택하는 것을 의미

### 6.2.1 간단한 예제

- 트리 분할
    - **모든 가능한 분할 계산**: 각 독립 변수의 가능한 모든 분할 지점을 고려
    - **분할 기준 선택**: 각 분할 지점에서 분할 후의 불순도 감소(예: 지니 지수, 엔트로피 감소, 잔여 제곱합 감소)를 계산
    - **최적의 분할 선택**: 불순도가 가장 많이 감소하는 분할 지점을 선택하여 데이터를 두 개의 하위 집합으로 분할
- 노드 예시
    - `2) borrower_score>=0.575 878  261 paid off (0.7027335 0.2972665)`
        - 878개의 전체 데이터에서 261개의 오분류 의미
        - 괄호 안의 숫자: 해당 데이터(노드 2의 조건을 만족하는)에서 paid_off, default 의 비율이 각각 0.7027 ~, 0.2972~ 라는 뜻
        - 확률이 높은 클래스가 paid off라서 써있는 것

### 6.2.2 재귀 분할 알고리즘

- 재귀분할: 의사 결정 트리를 만드는 알고리즘
    - 예측변수 값을 기준으로 데이터를 반복적으로 분할
    - 상대적으로 같은 클래스의 데이터들이 구분되도록 분할
- 시각화 예시
    
    ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%206.png)
    
    - 규칙 1(rule_number 1, 굵은 실선): 0.575 기준으로 분할
- 수도 코드
    - 응답변수 Y, 예측변수 P개 (x_1, …, x_p)
    - 어떤 파티션(분할 영역) A에 대해, A를 두 개의 하위 분할 영역으로 나누기 위한 가장 좋은 재귀적 분할 방법
        1. 각 예측변수 X_j에 대해
            1. X_j에 해당하는 각 변수 s_j에 대해 
                1. A에 해당하는 모든 레코드를 X_j < s_j인 부분과 나머지 X_j ≥ s_j인 부분으로 나눈다 
                    - c.f. X_j가 범주형이면 요인별로 나눈다
                2. A의 각 하위 분할 영역 안에 해당 클래스의 동질성을 측정 
                    - 각 영역은 해당 영역에 속한 응답변수들의 다수결 결과(Y값이 가장 많이 나온 걸로) 예측 결과 결정
            2. 하위 분할 영역 내에서 클래스 동질성이 가장 큰 s_j 값을 선택
        2. 클래스 동질성이 가장 큰 변수 X_j와 s_j 값을 선택 
- 알고리즘의 재귀 부분
    1. 전체 데이터를 가지고 A를 초기화
    2. A를 두 부분 A_1, A_2로 나누기 위해 분할 알고리즘 적용
    3. A_1, A_2 각각에서 2번 과정을 반복
    4. 분할을 더 해도 하위 분할 영역의 동질성이 개선되지 않을 정도로 충분히 분할을 진행했을 때, 알고리즘 종료 
- 최종 결과는 P 차원
    - 아까 시각화에서는 예측변수를 2개만 사용했기 때문에 그래프가 2차원
    - 예측변수가 P개면 최종 결과는 P차원에서 존재 (p차원 벡터 maybe?)
- 이진 분류의 경우, 하위 분할 영역에 존재하는 0과 1의 개수에 따라 확률값을 구할 수도 있음
    - P(Y=1) = (파티션 내에서) Y=1이 나온 데이터 개수  / 전체 데이터 개수 (파티션 크기)
    - P(Y=1) > 0.5인 경우 1로 예측
        - cutoff 정해서 이진 결정 가능

### 6.2.3 동질성과 불순도 측정하기

- 트리 모델링: 분할 영역 A(행 집합)을 재귀적으로 만드는 과정
- 클래스 순도(class purity): 각 분할 영역에 대한 동질성
- 예측의 정확도: 해당 파티션 내에서 잘못 분류된 레코드의 비율 p (0~0.5(순수 랜덤 추측, 이진 분류에서)) 사이
    - 불순도 측정에는 부적합
- Gini impurity, entropy가 대표적인 불순도 측정 지표
    - 클래스가 2개 이상인 분류 문제에도 적용 가능. 책에서는 설명을 위해 이진 예제 사용
    - $I(A) = p(1-p)$
- Entropy
    - $I(A) = -plog_2(p) - (1-p)log_2(1-p)$
- 정확도가 높은 부분에서는 지니 불순도와 엔트로피 측정값이 비슷하게 높아진다
    - 지니 불순도의 경우 스케일은 조정했음
    
    ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%207.png)
    
- c.f. 지니 계수 vs. 지니 불순도
    - 지니 계수는 이진 분류 문제로 한정. AUC와 관련이 있는 용어
- 분할 알고리즘에서도 불순도 측정 지표 사용
    - 분할로 만들어지는 각 영역에 대해 불순도 측정
    - 가중평균 계산 → 가장 낮은 가중평균을 보이는 분할 영역을 선택
        - 각 자식 노드의 불순도에 해당 노드의 데이터 포인트 수를 가중치로 곱한다고 함

### 6.2.4 트리 형성 중지하기

- 트리가 커질 수록
    - 분할 규칙 세분화
    - 실제 믿을 만한 관계들을 확인하는 큰 규칙 → 노이즈를 반영하는 아주 작은 규칙을 만드는 단계로 변화
    - 순도가 완전히 100%가 될 때까지 다 자란 트리는 학습한 데이터에 대해 100%의 정확도 갖게 됨
        - 그러나 오버피팅. 새로운 데이터 분류 시 방해가 됨
- 언제 트리 성장을 멈춰야 하는지 결정하는 방법 필요
    - 새로 들어오는 데이터에 대해 좋은 일반화 성능을 얻기 위해
    - 분할을 통해 얻어지는 하위 영역(leaf node) 크기가 너무 작다면 분할 중지
        - 최소 분할 영역 크기나 leaf node 크기 조절 가능
            - r: rpart 함수에서 minsplit, minbucket 과 같은 파라미터 이용
                - 기본값은 split은 20, bucket은 7
            - python: DecisionTreeClassifier에서는 인수 min_samples_split, min_samples_leaf 사용하여 제어
                - 기본값은 split 2, leaf는 1.
    - 새로운 분할 영역이 유의미한 정도로 불순도를 줄이지 않는다면 굳이 분할 X
        - r: rpart 함수에서 cp 이용 (complexity parameter)
        - python: DecisionTreeClassifier에서 min_impurity_decrease 파라미터 이용
            - 가중불순도 감소값에 따라 분할 제한
            - 값이 작을 수록 트리는 더 복잡해짐
    - 교차 검증을 통해 모델 파라미터를 체계적으로 변경 + 가지치기를 통해 트리를 수정
- R에서 트리 복잡도 제어
    - cp가 매우 작으면 노이즈까지 학습하여 오버피팅 ↔ 너무 크면 트리가 작아 예측 능력 미미
    - rpart 함수의 기본 설정은 0.01로 다수 큰 값. 사전 탐색 분석에서 몇 가지 값을 테스트 해보기
    - Cross-validation
        1. 데이터를 학습용, validation 용 데이터로 분할
        2. 학습 데이터로 트리 성장
        3. 트리를 단계적으로 가지치기
            - 매 단계에서 **학습데이터**를 이용해 cp 기록
        4. validation dataset에 대해 최소 loss를 보이는 cp 기록 
        5. 데이터를 다시 train, valid set으로 분할 후 2~4 반복 
        6. 5를 여러번 반복한 후에 각 트리에서 최소 loss를 보이는 cp 값의 평균 계산
        7. 원래 whole 데이터를 이용해서 6에서 구한 cp 최적값을 가지고 트리를 만든다 
    - rpart > `cptable`
        - cp값과 이에 해당하는 cross-validation test loss(xerror)가 보이는 표를 얻을 수 있음
        - 예시 코드
            
            ```sql
            # rpart 모델 생성
            model <- rpart(Species ~ ., data=iris, control=rpart.control(cp=0.01))
            
            # 가지치기 표 출력
            print(model$cptable)
            
            # cptable을 데이터프레임으로 변환하여 저장
            cp_table <- as.data.frame(model$cptable)
            print(cp_table)
            ```
            
- 파이썬에서 트리 복잡도 제어하기
    - `cpp_alpha`
        - 기본 값은 0 = 가지치기 없음
        - 값이 높아질 수록 가지치기 많이해서 트리가 작아짐
    - `GridSearchCV` 사용해서 최적의 값 찾기 가능
        - 예시 코드
            
            ```sql
            import numpy as np
            import pandas as pd
            from sklearn.model_selection import GridSearchCV
            from sklearn.tree import DecisionTreeClassifier
            from sklearn.datasets import load_iris
            from sklearn.model_selection import train_test_split
            
            # 데이터 로드
            iris = load_iris()
            X = iris.data
            y = iris.target
            
            # 훈련 및 테스트 데이터 분할
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # 하이퍼파라미터 그리드 설정
            param_grid = {
                'criterion': ['gini', 'entropy'],
                'max_depth': [None, 10, 20, 30],
                'min_samples_split': [2, 10, 20]
            }
            
            # 의사결정 트리 모델 생성
            dt = DecisionTreeClassifier()
            
            # GridSearchCV 설정
            grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)
            
            # 모델 학습
            grid_search.fit(X_train, y_train)
            
            # 최적의 하이퍼파라미터 출력
            print("Best parameters found: ", grid_search.best_params_)
            
            # 최적의 모델로 예측 및 성능 평가
            best_model = grid_search.best_estimator_
            accuracy = best_model.score(X_test, y_test)
            print(f"Test set accuracy: {accuracy:.4f}")
            ```
            
        - max_depth(5~30), min_samples_split(20~100) 등 다양한 모델 파라미터로 트리 크기 제어 가능
        - GridSearchCV는 모든 조합에 대한 와전 검색과 교차 검증을 결합한 편리한 방법

### 6.2.5 연속값 예측하기

- 회귀분석
- 분류랑 비슷하지만 차이점
    - 각 하위 분할 영역에서 평균으로부터 편차들을 제곱한 값을 이용
        - 편차: 각 하위 영역에서 데이터 포인트들의 실제 값과 해당 노드의 평균 값 간의 차이합
    - 예측 성능은 RMSE 이용
- python에서는 sklearn.tree.DecisionTreeRegressor

### 6.2.6 트리 활용하기

- 트리모델은 블랙박스가 아니다
    - 데이터 탐색을 위한 시각화 가능
        - 어떤 변수가 중요하고, 변수 간 관계 보여줌
        - 예측변수들 간의 비선형 관계를 담아낼 수 있음
    - 규칙들의 집합
        - 비전문가들과도 커뮤니케이션 효과적
- 예측의 경우 다중 트리에서 나온 결과가 단일 트리 이용보다 강력
    - 랜덤 포레스트, 부스팅 트리 알고리즘 우수
    - 그러나 단일 트리의 장점을 상실하게 됨

## 6.3 배깅과 랜덤 포레스트 🎒🏞️

- 앙상블 방법의 가장 간단한 버전
    1. 주어진 데이터에 대해 예측 모델 만들고 예측 결과 기록
    2. 같은 데이터에 대해 여러 모델을 만들고 결과 기록
    3. 각 레코드에 대해 예측된 결과들의 평균(가중 평균, 다수결 등)을 구한다 
- 가장 많이 사용되는 앙상블 방법: 배깅, 부스팅
- 앙상블 기법이 트리 모델에 적용된 경우: 랜덤 포레스트, 부스팅 트리

### 6.3.1. 배깅

- 배깅(bootstrap aggregating)의 줄임말
- 응답변수 Y, 예측변수 P개(칼럼 P개) (X_1, …, X_P), 레코드 수(칼럼별 행 수) N개
- 앙상블은 다양한 모델을 정확히 같은 데이터에 대해 구함 vs. 배깅은 매번 부트스트랩 재표본에 대해 새로운 모델 생성
- 기본 알고리즘
    1. 만들 모델의 개수 M과 모델을 만드는 데 사용할 레코드의 개수 n(n < N)의 값을 초기화
        - 반복 변수 m=1으로 설정
    2. train data로부터 복원 추출!! 방법으로 n개의 부분 데이터 Y_m과 X_m을 부트스트랩 재표본 추출
        - X 뿐만 아니라 Y도 복원 추출
        - 레코드 부트스트랩 샘플링
    3. 의사 결정 규칙 $\hat{f_m}(X)$를 얻기 위해, Y_m과 X_m을 이용해 모델 학습
    4. m = m + 1로 모델 counter를 하나 늘린다
        - m ≤ M 이면 다시 2단계로  == m이 M이 되면 종료
    - $\hat{f_m}(X)$이 Y=1인 경우의 확률을 예측한다고 했을 때, 배깅 추정치
        
        $\hat{f} = \frac{1}{M}(\hat{f_1}(X)+\hat{f_2}(X)+...+\hat{f_M}(X))$
        

### 6.3.2 랜덤 포레스트

- 기본 트리 알고리즘 + 배깅 + 각 분할을 위한 변수(Feature)의 부트스트랩 샘플링
    - 레코드를 표본 추출할 때, 변수 역시 샘플링
    - 일반적인 의사 결정 트리
        - 하위 분할 영역 A를 만들 때, 지니 불순도와 같은 기준값이 최소화되도록 변수와 분할 지점 결정
    - 랜덤 포레스트에서는 알고리즘의 각 단계마다 고를 수 있는 변수가 랜덤하게 결정된 전체 변수들의 부분 집합에 한정됨
    - 알고리즘
        1. 전체 데이터로부터 부트스트랩 샘플링(복원 추출)
            - 레코드 부트스트랩 샘플링인듯
        2. 첫 분할을 위해 비복원 임의표본 추출로 p(p<P)개의 변수를 샘플링
            - 칼럼 부트스트랩 샘플링
        3. 샘플링된 변수 $X_{j(1)}, X_{j(2)}, ..., X_{j(p)}$에 대해 분할 알고리즘 적용 
            1. $X_{j(k)}$의 각 변수 $s_{j(k)}$에 대해 
                1. 파티션 A에 있는 레코드들을 $X_{j(k)} < s_{j(k)}$ 인 하위 영역과 $X_{j(k)} >= s_{j(k)}$ 인 하위 영역으로 분할 
                2. A의 각 하위 영역 내부의 클래스의 동질성 측정
            
            c.f. j는 몇 번째 트리(모델)에 사용되는 train data인지를 의미하는 듯? 배깅에서 m의 역할이 아닐까 함 
            
        4. 분할 영역 내부의 클래스 동질성을 최대로 하는 $X_{j(k)}$, $s_{j(k)}$ 값을 선택 
        5. 다음 분할을 진행하기 위해, 2단계부터 시작해 이전 단계들을 반복
        6. 트리가 모두 자랄 때까지 위와 같은 분할 과정 반복  
        7. 1단계로 돌아가 또 다른 부트스트랩 표본을 추출해 같은 과정 반복
            - 1~6이 하나의 트리를 키우는 과정이라고 생각하면 됨
    - 칼럼 선택 개수
        - 전체 변수가 P개 이면 보통 $\sqrt{P}$개의 칼럼을 선택하는 것이 국룰
- 트리 개수에 따른 OOB 에러 변화
    - 트리가 무조건 많아질 수록 에러가 낮아지는 것은 아니지만, 처음 몇 개에서는 많아질 수록 에러가 낮아지는 경향이 나타나긴 한다
        
        ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%208.png)
        
- 예측 결과 시각화
    - 랜덤 포레스트는 일종의 블랙박스 모델
        - 단일 트리보다 예측 성능 높지만 직관적인 해석은 불가
        - 예측 결과도 지저분하다는데 그래프는 깔끔 ㅋㅋ
    - 신용점수(X축)가 높은 사람 중에도 대출을 다 갚지 못할 거라는 예외 예측 결과가 있음 (범례 바로 밑)
        - 랜덤 포레스트에 의한 오버피팅의 위험성
    
    ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%209.png)
    

### 6.3.3 변수 중요도

- 데이터의 행, 칼럼(feature) 개수가 많은 상황에서 예측 모델 만들 때
    - 아래 사항을 자동으로 결정하는 능력이 장점
        - 다수의 칼럼 중 어느 게 중요한지
        - 칼럼 간 존재하는 상관관계 항에 대응되는 복잡한 관계
- 변수별 변화에 따른 성능 변화 시각화
    - 분류 정확도의 평균 감소량
        - 원래 데이터로 RF 모델 학습이 완료된 상황에서 측정
            - 원래 데이터에 대한 예측 성능 vs. 특정 변수(칼럼) 값을 무작위로 섞어 놓은 다음 그 데이터에 대한 예측 성능 비교
        - 모든 레코드에 대해 평균적으로 성능 변화가 얼마나 있었는지
            - 크면 클 수록 모델에 해당 변수(칼럼)이 미치는 변화가 크다는 것
        - 우리 예시의 경우 borrower_score의 영향이 가장 큰 것으로 나옴
            
            ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%2010.png)
            
    - 노드 불순도의 평균 감소량
        - 원래 데이터로 RF 모델을 학습시키는 동안, 각 노드에서 사용된 변수들의 지니 불순도 감소량을 누적하여 기록
            - 이번 분할에 사용할 수 있는 $\sqrt{p}$개의 칼럼은 랜덤으로 정해짐
            - 분할 전 모든 칼럼에 대해 지니 불순도 감소량 기록
                - 해당 칼럼의 특정 기준값으로 노드를 분할하기 전후의 불순도 비교
            - 그 중에서 가장 감소량이 큰 변수-기준값 pair가 해당 분할의 기준이 됨
        - 학습이 완료된 후, 각 변수의 중요도는 누적된 감소량을 바탕으로 계산
            - 실제 분할 기준이 된 변수들에 대해서 그 분할로 얼마나 감소했는지를 노드별, 트리별로 누적 → 그 변수로 인해 분할 몇 번했는지로 나누어 평균 계산
            - 특정 변수가 분할 기준으로 사용된 모든 경우에서의 지니 불순도 감소량의 합 /  그 변수가 사용된 분할 횟수로 나눈 값
        - c.f. 헷갈리는 개념
            - 트리 분할 ⊂ 트리 학습
            - 한번의 트리 분할 시 어떤 칼럼 사용할지 어떻게 결정?
                1. 랜덤 포레스트에서는 모든 트리에 대해서 모든 분할 시마다 무작위로 변수를 선택
                2. 그렇게 골라진$\sqrt{p}$개의 칼럼 중에서, 각 변수에 대해 다양한 분할 기준 시도
                3. 지니 불순도나 엔트로피 같은 기준으로 최적의 분할(최적 변수, 분할 기준) 선택 
            - 노드별 지니 불순도 계산 방법
                
                ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%2011.png)
                
        - 우리 예시에서는 payment_inc_ratio가 가장 영향력이 높은 것으로 나옴
            
            ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%2012.png)
            
- 변수 중요도 측정 방법
    1. 변수의 값을 랜덤하게 섞었다면 모델의 정확도가 감소하는 정도를 측정 (type=1)
        - 변수를 랜덤하게 섞는 것: 해당 변수가 예측에 미치는 모든 영향력을 제거
        - 정확도는 OOB 데이터로부터 계산 = 교차타당성 검사와 같은 효과
    2. 특정 변수를 기준으로 분할이 일어난 모든 노드에서 불순도 점수의 평균 감소량을 측정 (type=2)
        - 해당 변수를 포함하는 것이 노드의 순도를 얼마나 개선하는지
        - 학습 데이터를 기반으로 측정 → OOB 데이터 가지고 계산한 것에 비해 신뢰도 적음
- 정확도 감소량 vs. 노드 불순도
    - rF 함수는 지니 불순도만 활용
    - 모델의 정확도는 추가적인 계산(랜덤 순열 조합으로 데이터를 섞고, 이 데이터를 이용해 예측 결과 예측)이 필요
    - 지니 불순도는 알고리즘상에서 부차적으로 얻어지는 결과물
    - 계산 복잡도가 중요하면 정확도 감소량까지 추가로 계산해서 얻는 이득이 미미

### 6.3.4 하이퍼파라미터

- 성능을 조절할 수 있는 손잡이가 달린 일종의 블랙발스 알고리즘
    - 손잡이 = 하이퍼파라미터
    - 모델 학습 전에 미리 설정. 학습 과정 중에 최적화 되는 대상이 아님
- nodesize/min_sample_leaf
    - 말단 노드의 크기
    - 기본 설정: R분류(1), R회귀(5), sklearn 분류, 회귀(1)
- maxnodes/max_leaf_nodes
    - 각 결정 트리에서 전체 노드의 최대 개수
    - 파이썬에서는 지정. R에서는 제한 없고 nodesize 제한 설정에 따라 가장 큰 트리의 크기 결정
    - maxnodes = 2 * max_leaf_nodes - 1
- 기본설정으로는 오버피팅에 빠지기 쉬움
    - min_sample_leaf를 크게 하거나, max_leaf_nodes 수를 설정하면 더 작은 트리를 얻게 되고, 거짓 예측 규칙들을 만드는 것을 피할 수 있음

## 6.4 부스팅

- 부스팅: 모델들을 앙상블 형태로 만드는 일반적인 기법
    - 이전 모델이 갖는 오차를 줄이는 방향으로 다음 모델을 연속적으로 생성
- 배깅 vs. 부스팅
    - 배깅은 상대적으로 튜닝이 거의 필요하지 않음. 부스팅은 적용하고자 하는 문제에 따라 주의 필요
    - 배깅은 혼다 어코드 vs. 부스팅은 포르쉐 ;;
- 가장 자주 사용되는 변형된 형태의 부스팅 알고리즘들
    - 에이다부스트, 그레디언트 부스팅, 확률적 그레디언트 부스팅 (#1)
    - 파라미터를 잘 설정하면 확률적 그레디언트 부스팅으로 RF와 유사한 동작을 할 수 있도록 할 수 있다고 함

### 6.4.1 부스팅 알고리즘

- Adaboost 알고리즘
    1. 피팅할 모델의 개수 M을 설정
        - 초기화: 반복 횟수(model count) m = 1, 관측 가중치 w_i = 1/N (i=1, 2, …, N)
        - 앙상블 모델 $\hat{F}_0$ = 0
    2. 관측 가중치 $w_1, w_2, .., w_N$을 이용해 모델 $\hat{f}_m$을 학습
        - 잘못 분류된 관측치에 가중치를 적용한 합을 의미하는 가중 오차 $e_m$이 최소화되도록 학습
    3. 앙상블 모델에 다음 모델 추가 
        - $\hat{F}_m = \hat{F}_{m-1} + \alpha_{m}\hat{f}_{m}$
        - $\alpha_{m} = \frac12 \log\frac{1 - e_m}{e_m}$
    4. 잘못 분류된 입력 데이터에 대한 가중치를 증가하는 방향으로 가중치 $w_1, w_2, ..., w_N$을 업데이트
        - $\alpha_m$에 따라 증가 폭이 결정되며, $\alpha_m$이 클 수록 가중치가 더 커진다
            
            = 모델의 오차가 낮을 수록 더 큰 가중치를 부여 
            
    5. 모델 반복 횟수를 m = m + 1 으로 증가시키고 m ≤ M 이면 다시 2단계로 돌아간다 
- Adaboost 요약
    - 이 과정을 통해 얻은 부스팅 추정치 $\hat{F} = \alpha_1\hat{f}_1 + \alpha_2\hat{f}_2 + ... + \alpha_M\hat{f}_M$
        - 제일 초기 모델인 $a_1\hat{f}_1$의 경우 모든 레코드가 같은 가중치를 부여 받은 상황에서 학습한다
    - 잘못 분류된 관측 데이터에 가중치 증가 → 현재 성능이 제일 떨어지는 데이터에 대해 더 집중해서 학습을 하도록 하는 효과
    - chat gpt 요약
        
        1.	초기 가중치 설정: 모든 데이터 포인트에 동일한 가중치 부여.
        
        2.	약한 학습기 훈련: 가중치를 반영하여 약한 학습기 훈련.
        
        - 약한 학습기 = decision tree = $a_m\hat{f}_m$ = $\hat{F}_m$
        
        3.	오류율 계산: 학습기의 예측 오류율 계산.
        
        4.	학습기 가중치 계산: 학습기의 중요도 계산.
        
        5.	가중치 업데이트: 틀린 예측의 데이터 포인트에 대한 가중치 증가.
        
        6.	반복: 필요한 수의 학습기를 얻을 때까지 반복.
        
        7.	최종 예측: 모든 학습기의 예측을 가중치에 따라 결합하여 최종 예측 수행.
        
- Gradient boosting
    - Ada에서 가중치를 조정하는 과정 대신, Gradient 에서는 비용함수를 최적화하는 접근법 사용
        - ada 기본에서는 학습률 도입되지 않고, gradient류에서는 학습률이 도입됨
    - 모델이 유사 잔차(pseudo residual)을 학습하도록 함
        - 잔차가 큰 데이터를 더 집중적으로 학습하는 효과
        - c.f. 그래서 결국은
            - 각 단계에서 손실 함수를 최소화하기 위해 새로운 트리가 추가되는데
            - 이 과정에서 트리의 분할 규칙이 손실 함수의 기울기를 따라 조정되어
            - 잔여 오차를 줄이는 방향으로 예측값이 변화
    - chat gpt 추가 설명
        - 전체 과정 요약
            
            ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%2013.png)
            
        - 유사 잔차
            - 잔차는 보통 예측 값과 실제 값의 차이로 정의되지만, 유사잔차는 특정 손실 함수에 따라 계산된 기울기로 이해
                - 손실함수: 모델의 예측값과 실제 값 사이의 차이를 수치적으로 측정하는 함수
                - 손실 함수의 기울기는 손실 함수를 예측 값에 대해 편미분하여 계산
            - 모델이 현재 상태에서 더 나아지기 위해 어떤 방향으로 가야 하는지를 나타내는 신호
                - 예측값이 약간 변했을 때 손실 함수가 어떻게 변하는지
                - 모델이 더 나은 예측을 위해 어떤 방향으로 예측값을 조정해야 하는지를 알려주는 정보
        - 잔차 계산
            - 손실함수의 그래디언트
                
                ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%2014.png)
                
                - 손실함수
                    - 회귀에서는 MSE, 분류에서는 cross-entropy나 log loss
                - 그래디언트
                    - 다변수 함수에서 각 변수에 대한 편미분 값을 모은 벡터
                    - 손실 함수의 그래디언트는 각 데이터 포인트의 예측 값에 대한 손실 함수의 편미분 값을 포함한 벡터로 표현
                    - 레코드(행) 단위 편미분
                        
                        ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%2015.png)
                        
                    - 그래디언트 효용
                        
                        ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%2016.png)
                        
                        - 회귀/분류에서 편미분 식
                            
                            ![Untitled](6%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%8C%E1%85%A5%E1%86%A8%20%E1%84%86%E1%85%A5%E1%84%89%E1%85%B5%E1%86%AB%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20f2a270e183514f21aa9276ccf1c0cffd/Untitled%2017.png)
                            
- Stochastic Gradient Boosting
    - RF에서와 유사하게, 매 단계마다 데이터와 예측변수를 샘플링하는 식으로 그래디언트 부스팅에 랜덤한 요소를 추가
        - 트리마다 행 샘플링해서 training data subset을 얻음
        - 여기서 유사 잔차 줄이는 방식으로
    - c.f. RF vs. SGB
        
        
        | 항목 | RF | SGB |
        | --- | --- | --- |
        | 목적 | 주로 분산을 줄여서 모델의 안정성을 높이기 위한 방법 | 편향을 줄여 예측 성능을 점진적으로 개선 |
        | 학습 과정 | 여러 개의 결정 트리를 독립적으로 학습 | 트리들을 순차적으로 학습시키며, 각 트리는 이전 트리의 오류(잔차)를 줄이기 위해 학습 |
        | 앙상블 방식 | 이 트리들의 예측을 평균 내거나 투표하여 최종 예측 | 최종 예측은 각 트리의 예측을 가중 합하여 만들며, 이전 트리의 오류를 줄이는 방향으로 가중치가 조정 |

### 6.4.2 XG 부스트

- 부스팅 방법 가운데 대중적으로 가장 많이 사용되는 오픈 소스 소프트웨어
- 가장 중요한 파라미터
    - subsample
        - 각 iteration마다 샘플링할 입력 데이터 비율 조정
            - OOB를 얼마만큼 떼놓을 것인지
        - subsample 설정에 따라 비복원 추출로 샘플링한다는 점만 빼면 부스팅은 마치 랜덤 포레스트 같이 동작
            - RF는 bootstrap이라 복원 추출
    - eta
        - 부스팅 알고리즘에서 $\alpha_m$에 적용되는 축소 비율 결정
        - 가중치의 변화량을 낮추어 오버피팅을 방지하는 효과
            - 기본 adaboost에서는 학습률이 도입되지 않지만, alpha에 eta를 줄 수도 있는 듯
                
                
                $\alpha_m = \eta \cdot \frac{1}{2} \ln\left(\frac{1 - \epsilon_m}{\epsilon_m}\right)$